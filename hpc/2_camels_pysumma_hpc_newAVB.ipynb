{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "# records the time at this instant\n",
    "# of the program\n",
    "runtime_start = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Running the SUMMA setups on HPC\n",
    "In this notebook, we run SUMMA using pysumma, on the setups we created in the previous notebook.\n",
    "This is the HPC version of the notebook and only runs the full problem, the most complex problem.\n",
    "The last section of the notebook computes summary statistics of the output to be used in the next notebook. In this HPC notebook, you cannot see the summary statistics calculations as the code is on the HPC system.\n",
    "\n",
    "The complexity choice is\n",
    " - 3)   `lhs_config_prob = 1`: 8 different configurations with the default and the exploration of the parameter space.\n",
    "\n",
    "\n",
    "Less complex choices are available only in the local (non-HPC) version of the notebook, and not this notebook. These are, in order of increasing complexity: \n",
    " - 1)   `default_prob = 1`: the \"default\" configuration with the \"default\" parameters. By \"default\" we mean whatever you chose in the summa setup files. \n",
    " - 2a) `lhs_prob = 1`: the default configuration with exploration of the parameter space.\n",
    " - 2b) `config_prob = 1`: the default parameters with 8 different configurations (choices that have been seen to affect the model output in previous research) \n",
    " \n",
    "\n",
    "Eight iterations of each loop are run for each problem, to cover a truth run and the 7 forcings each held to a daily constant in turn.\n",
    " \n",
    "You can make the problem run for fewer years to lower computational costs, changing `str(the_start)` and `str(the_end)`. You can test with as little as a day between `the_start` and `the_end`, but if you want to make the plots in this notebook and run the next notebook you will need >1 year (1 year is considered intialization period). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2_1 Preliminary steps\n",
    "### 2_1_1 Check the enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Check that we loaded a correct environment. This should show pysumma version 3.0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda list summa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2_1_2 Import libraries\n",
    "Load the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pysumma as ps\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2_1_3 Choose HPC machine\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b> Choose the HPC  </b> Choose which HPC machine to use: either \"expanse\" or \"keeling\". We recommend using 'expanse' as it is a more poweful hpc leading to faster calculations.  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " hpc_machine = \"expanse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2_2 Set up paths to SUMMA configuration files for CAMELS basins\n",
    "### 2_2_1 Set up paths to SUMMA configuration files\n",
    "Set up the paths and regionalize the paths in the configuration files that SUMMA will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_folder = os.path.join(os.getcwd(), 'summa_camels')\n",
    "settings_folder = os.path.join(top_folder, 'settings')\n",
    "ps_working = os.path.join(top_folder, '.pysumma')\n",
    "regress_folder = os.path.join(os.getcwd(), 'regress_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make installTestCases_local.sh executable and navigate to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd {top_folder}; chmod +x installTestCases_local.sh; ./installTestCases_local.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of HRUs\n",
    "attrib = xr.open_dataset(settings_folder+'/attributes.nc')\n",
    "the_hru = np.array(attrib['hruId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "##  2-3 Make problem complexity choices\n",
    " \n",
    "The only complexity choice you can make is to run a different length time-period. It is pre-populated to run 6 years as in the paper. You also need to choose how long the initialization period is for the error calculations. We suggest 183 to 365 days, with at least 1 more year of simulation. So for example, if you run 18 months of simulation you should choose your initialization to be 183 days.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prob = 0    #this should be 0 in this HPC notebook\n",
    "lhs_prob = 0        #this should be 0 in this HPC notebook\n",
    "config_prob = 0     #this should be 0 in this HPC notebook\n",
    "lhs_config_prob = 1 #this should be 1 in this HPC notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b> Select simulation period and initialization days:  </b> Select simulation period and initialization days  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_start = '1990-10-01 00:00' #pre-populated to '1990-10-01 00:00' as in the paper.\n",
    "the_end =   '1996-09-30 23:00' #pre-populated to '1996-09-30 23:00' as in the paper.\n",
    "initialization_days = 365 #pre-populated to 365 days as in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2_4 Setup HPC\n",
    "\n",
    "This code sets up the HPC by setting the executable, writing the start and end files to the filemanager, and setting up workspaces on the HPC.\n",
    "\n",
    "**TODO: Mention more details about the HPC that we use either here or at the beginning.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define executable\n",
    "executable =  '/usr/bin/summa.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write simulation start and end dates in the template_file_manager.1hr.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_line_startwith(lines, flag, new_line_replacement):\n",
    "    for i in range(len(lines)):\n",
    "        if lines[i].startswith(flag):\n",
    "            print(lines[i])\n",
    "            lines[i] = new_line_replacement\n",
    "    return lines\n",
    "temple_filemanager = os.path.join(settings_folder, \"template_file_manager.1hr.txt\")\n",
    "print(temple_filemanager)\n",
    "with open(temple_filemanager, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    new_lines1 = replace_line_startwith(lines, \"simStartTime\", \"simStartTime         '{the_start}' !\\n\".format(the_start=the_start))\n",
    "    new_lines2 = replace_line_startwith(new_lines1, \"simEndTime\", \"simEndTime           '{the_end}' !\\n\".format(the_end=the_end))\n",
    "f = open(os.path.join(settings_folder, \"template_file_manager.1hr.txt\"), \"w\")\n",
    "#f.writelines(new_lines1)\n",
    "f.writelines(new_lines2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Special json encoder for numpy types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\"\n",
    "    \n",
    "    Credit:\n",
    "    https://stackoverflow.com/questions/26646362/numpy-array-is-not-json-serializable\n",
    "    \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle summa_camels.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf summa_camels.zip\n",
    "workspace_dir = os.path.join(os.getcwd(), 'workspace')\n",
    "!mkdir -p {workspace_dir}\n",
    "unzip_dir = tempfile.mkdtemp(dir=workspace_dir)\n",
    "model_folder_name = \"summa_camels\"\n",
    "model_folder = os.path.join(unzip_dir, model_folder_name)\n",
    "shutil.make_archive(model_folder_name, 'zip', os.getcwd()+\"/summa_camels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2_5 Exploring the parameter  clibration space with a Latin Hypercube\n",
    "\n",
    "Here we make parameter sets selected by using a Latin Hypercube to get 10 different parameter sets for every HRU, in order to explore the calibration space. The default parameter space is still included. This exploration will show us if the results of forcing importance could change after calibration. \n",
    "\n",
    "We change only the parameters that are usually calibrated. You can remove parameters if you were not planning to ever calibrate them away from their defaults (likewise you could add parameters).\n",
    "\n",
    "The absolute minimums and maximums will break simulations and zero out variables, so we do not use those, we stay at the 5% level away from the extremes. Also, there are some constraints on the parameters that must be followed, they are:\n",
    "\n",
    "* heightCanopyTop   > heightCanopyBottom\n",
    "* critSoilTranspire > theta_res\n",
    "* theta_sat         > critSoilTranspire\n",
    "* fieldCapacity     > theta_res\n",
    "* theta_sat         > fieldCapacity\n",
    "* theta_sat         > theta_res\n",
    "* critSoilTranspire > critSoilWilting\n",
    "* critSoilWilting   > theta_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_prob==1 or lhs_config_prob==1: from pyDOE import lhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    file_manager = settings_folder+'/file_manager_truth.txt'\n",
    "    s = ps.Simulation(executable, file_manager)\n",
    "    s.manager['simStartTime'] = str(the_start)\n",
    "    s.manager['simEndTime'] = str(the_end)  \n",
    "    #Before running the ensemble that changes parameters we must write the original simulation's parameters.\n",
    "    s.manager.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the default, min, and max as in /settings.v1/localParamInfo.txt and /settings.v1/basinParamInfo.txt\n",
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    param_calib_hru = ['albedoRefresh', 'aquiferBaseflowExp', 'aquiferBaseflowRate', 'frozenPrecipMultip', 'heightCanopyBottom','heightCanopyTop', 'k_macropore', \n",
    "                   'k_soil', 'qSurfScale', 'summerLAI', 'tempCritRain', 'theta_sat', 'windReductionParam'] \n",
    "    param_calib_gru = ['routingGammaScale', 'routingGammaShape']\n",
    "\n",
    "    for k in param_calib_hru:\n",
    "        print(s.global_hru_params[k])\n",
    "    for k in param_calib_gru:\n",
    "        print(s.global_gru_params[k]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    bounds_hru = np.full((len(param_calib_hru),3),1.0)\n",
    "    bounds_gru = np.full((len(param_calib_gru),3),1.0)\n",
    "    for i,k in enumerate(param_calib_hru): bounds_hru[i,]= s.global_hru_params.get_value(k)[0:3]\n",
    "    for i,k in enumerate(param_calib_gru): bounds_gru[i,]= s.global_gru_params.get_value(k)[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bounds and expand to size of LHS runs\n",
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    numl = 10\n",
    "    num_vars =  len(param_calib_hru) + len(param_calib_gru)\n",
    "    names =  param_calib_hru + param_calib_gru\n",
    "    bounds =  np.concatenate((bounds_hru, bounds_gru), axis=0)\n",
    "    par_def = dict(zip(names, np.transpose(np.tile(bounds[:,0],(len(the_hru),1))) ))\n",
    "    par_min = dict(zip(names, np.transpose(np.tile(bounds[:,1],(numl*len(the_hru),1))) ))\n",
    "    par_max = dict(zip(names, np.transpose(np.tile(bounds[:,2],(numl*len(the_hru),1))) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "We remove geographically distributed parameters from the default set, and then make the LHS parameter set plus deault from the above bounds. Set 0 will be the default parameter set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove geographically distributed parameters from default set\n",
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    distributed_val = par_def.pop('heightCanopyBottom')\n",
    "    distributed_val = par_def.pop('heightCanopyTop')\n",
    "    distributed_val = par_def.pop('k_soil')\n",
    "    distributed_val = par_def.pop('theta_sat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to obey parameter constraints\n",
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    param = xr.open_dataset(settings_folder+'/parameters.nc')\n",
    "\n",
    "    for i,h in enumerate(the_hru):\n",
    "        lb_theta_sat = max(param[['critSoilTranspire','fieldCapacity','theta_res']].isel(hru=i).values()).values\n",
    "        for j in range(0,numl): #say first numl belong to hru 0, second numl to hru 1, and so on\n",
    "            if (par_min['theta_sat'][j + i*numl]<lb_theta_sat): par_min['theta_sat'][j + i*numl]=lb_theta_sat\n",
    "\n",
    "    par_min['heightCanopyTop'] = par_max['heightCanopyBottom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a 5% buffer\n",
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    buff = {key: (par_max.get(key) - par_min.get(key))*0.05 for key in set(par_max) }\n",
    "    par_min = {key: par_min.get(key) + buff.get(key)*0.05 for key in set(buff) }\n",
    "    par_max = {key: par_max.get(key) - buff.get(key)*0.05 for key in set(buff) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate samples with Latin Hypercube Sampling, set seed by HRU ID so it is the same every time run\n",
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    lhd = np.empty(shape=(num_vars,numl*len(the_hru)))\n",
    "    for i, h in enumerate(the_hru):\n",
    "        np.random.seed(h) #if the hru ID is not a number this will not work\n",
    "        lhd[:,range(i*numl,(i+1)*numl)] = lhs(numl, samples=num_vars)\n",
    "    lhd = dict(zip(names,lhd))\n",
    "    samples = {key: par_min.get(key) + lhd.get(key)*(par_max.get(key) - par_min.get(key)) for key in set(par_max) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make parameter sets\n",
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    latin = {}\n",
    "    latin[str(0)] = {'trial_parameters': {key: par_def.get(key) for key in set(par_def) }}\n",
    "    for j in range(0,numl):\n",
    "            latin[str(j+1)] = {'trial_parameters': {key: samples.get(key)[np.arange(j, len(the_hru)*numl, numl)] for key in set(samples) }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2_6 Manipulating the configuration of the pysumma objects\n",
    "\n",
    "We need to run the parameter space with other model configurations, to see if the results seen on the default configuration hold true across the parameter space. The new configurations follow the exploration of [this paper](https://doi.org/10.1002/2015WR017200).\n",
    "\n",
    "Clark, M.P., Nijssen, B., Lundquist, J.D., Kavetski, D., Rupp, D.E., Woods, R.A., Freer, J.E., Gutmann, E.D., Wood, A.W., Gochis, D.J. and Rasmussen, R.M., 2015. A unified approach for process‐based hydrologic modeling: 2. Model implementation and case studies. Water Resources Research, 51(4), pp.2515-2542.\n",
    "\n",
    "Of the model configurations discussed in this paper, the decisions that made the most difference are:\n",
    "\n",
    " - `groundwatr` choice of groundwater parameterization as:\n",
    "   - `qTopmodl` the topmodel parameterization (note must set hc_profile = pow_prof and bcLowrSoiH = zeroFlux\n",
    "   - `bigBuckt` a big bucket (lumped aquifer model) in between the other two choices for complexity\n",
    "   - `noXplict` no explicit groundwater parameterization\n",
    " - `stomResist` choice of function for stomatal resistance as:\n",
    "   - `BallBerry` Ball-Berry (1987) parameterization of physiological factors controlling transpiration\n",
    "   - `Jarvis` Jarvis (1976) parameterization of physiological factors controlling transpiration\n",
    "   - `simpleResistance` parameterized solely as a function of soil moisture limitations\n",
    " - `snowIncept` choice of parameterization for snow interception as:\n",
    "   - `stickySnow` maximum interception capacity is an increasing function of temperature\n",
    "   - `lightSnow` maximum interception capacity is an inverse function of new snow density\n",
    " - `windPrfile` choice of wind profile as:\n",
    "   - `exponential` an exponential wind profile extends to the surface\n",
    "   - `logBelowCanopy` a logarithmic profile below the vegetation canopy\n",
    "\n",
    "Choices `bigBuckt`, `BallBerry`, `lightSnow`, and `logBelowCanopy` are the defaults that we have run already (see the decisions printed out in the previous cell). The paper showed choice of `groundwatr` affecting the timing of runoff and the magnitude of evapotranspiration, `stomResist` affecting timing and magnitude of evapotranspiration, `snowIncept` affecting the magnitude canopy interception of snow, and `windPrfile` affecting the timing and magnitude of SWE, and latent and sensible heat. We will not explore the `groundwatr` configurations here as the differences show up only in most simulated variables post-calibration. This study does not examine models calibrated for every set up of forcings configurations. Note, if you want to look at `qTopmodl`, you must set  `bcLowrSoiH` to `zeroFlux` (we will leave it at `drainage`) and `hc_profile` to `pow_prof` (we will leave it at `constant`). You can add other configuration choices here; this notebook and the next notebook will work properly (but it will make the computations take longer).\n",
    "\n",
    "We make use of pysumma `Simulation` objects and the `Ensemble` class, to set up suites of different model decisions (and add to this the different sets of parameters in the next section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_prob or lhs_config_prob==1:\n",
    "    file_manager = settings_folder+'/file_manager_truth.txt'\n",
    "    s = ps.Simulation(executable, file_manager)\n",
    "    s.manager['simStartTime'] = str(the_start)\n",
    "    s.manager['simEndTime'] = str(the_end)  \n",
    "    #Before running the ensemble that changes configuration we must write the original simulation's configuration.\n",
    "    s.manager.write()    \n",
    "    print(s.decisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_prob or lhs_config_prob==1:\n",
    "    #alld = {'groundwatr':np.array(['qTopmodl','bigBuckt']),\n",
    "    alld = {'stomResist':np.sort(np.array(['BallBerry','Jarvis'])),\n",
    "            'snowIncept':np.sort(np.array(['stickySnow', 'lightSnow'])),\n",
    "            'windPrfile':np.sort(np.array(['exponential','logBelowCanopy']))}\n",
    "    config = ps.ensemble.decision_product(alld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "The ensemble uses `++` as a delimiter to create unique identifiers for each simulation in the ensemble. The default configuration will be run again. We do this so that each finished SUMMA *.nc output file is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2_7 Run the full problem\n",
    "\n",
    "Now we can use the following code to make the full problem, exploring the parameter space and the configurations together. This is the problem that is run on the HPC.\n",
    "The next notebook in the series runs the most complete figures using this full problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2_7_1 Combine the decision sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make ensembles with parameter space (numl parameter sets plus 1 for default), should make 88\n",
    "if lhs_config_prob==1:\n",
    "    config_latin = {}\n",
    "    for key_config in config.keys():\n",
    "        c = config[key_config]\n",
    "        for key_latin in latin.keys():\n",
    "            l = latin[key_latin]\n",
    "            config_latin[key_config+key_latin] = {**c,**l}\n",
    "    print(len(config_latin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_latin1 = json.dumps(config_latin, cls=NumpyEncoder)\n",
    "config_latin = json.loads(config_latin1)\n",
    "list(config_latin.items())[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2_7_2 HPC setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_vars= [\"truth\",\n",
    "                'constant_airpres','constant_airtemp','constant_LWRadAtm',\n",
    "                'constant_pptrate','constant_spechum','constant_SWRadAtm',\n",
    "                'constant_windspd', ]\n",
    "config = {}\n",
    "idx = 0 \n",
    "for i, v in enumerate(constant_vars):\n",
    "    if v == \"truth\":\n",
    "        key = v\n",
    "    else:\n",
    "        key = 'run'+str(idx)\n",
    "        idx+=1\n",
    "    value = {'file_manager': '<PWD>/settings/file_manager_' + v +'.txt'}\n",
    "    config[key] = value\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_3 = {}\n",
    "for k, v in config.items():\n",
    "    for k2, v2 in config_latin.items():\n",
    "        v_copy = v.copy()\n",
    "        v_copy.update(v2)\n",
    "        config_3[\"{}_{}\".format(k, k2)] = v_copy\n",
    "len(config_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_3 = json.dumps(config_3, cls=NumpyEncoder)\n",
    "config_3 = json.loads(config_3)\n",
    "list(config_3.items())[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(config_3.items())[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Add explanation for the next cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if lhs_config_prob==1:\n",
    "    import tempfile\n",
    "    import shutil, os\n",
    "    workspace_dir = os.path.join(os.getcwd(), 'workspace')\n",
    "    !mkdir -p {workspace_dir}\n",
    "    unzip_dir = tempfile.mkdtemp(dir=workspace_dir)\n",
    "    model_folder_name = \"summa_camels\"\n",
    "    model_folder = os.path.join(unzip_dir, model_folder_name)\n",
    "    !unzip -o {model_folder_name}.zip -d {model_folder}\n",
    "    !rm -rf {model_folder}/output\n",
    "    !mkdir {model_folder}/output\n",
    "    !mkdir {model_folder}/output/constant\n",
    "    !mkdir {model_folder}/output/merged_day\n",
    "    !mkdir {model_folder}/output/truth\n",
    "    !mkdir {model_folder}/output/regress_data\n",
    "    with open(os.path.join(model_folder, \"output/regress_data/regress_param.json\"), 'w') as f:\n",
    "        json.dump({\"initialization_days\": initialization_days}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Add explanation for the next cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if lhs_config_prob==1:\n",
    "    import json\n",
    "    with open(os.path.join(model_folder, 'summa_options.json'), 'w') as outfile:\n",
    "        json.dump(config_3, outfile)\n",
    "\n",
    "    # check ensemble parameters    \n",
    "    print(\"Number of ensemble runs: {}\".format(len(config_3)))\n",
    "    print(json.dumps(config_3, indent=4, sort_keys=True)[:800])\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2_7_3 Submit model to HPC using CyberGIS-Compute Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Start HPC use  </b> (config_latin) config_latin SUMMA runs with each forcing </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_config_prob==1:\n",
    "    from job_supervisor_client import *\n",
    "    communitySummaSession = Session('summa', isJupyter=True)\n",
    "    communitySummaJob = communitySummaSession.job() # create new job\n",
    "    communitySummaJob.upload(model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b> Set the walltime  </b> If you are running for multiple basins, conisder increasing `\"walltime\" : 5` (which is the cap run time on the hpc in hours), to higher numbers so that the running time on the hpc does not extend this value. Otherwise, the job will fail. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_config_prob==1:\n",
    "    communitySummaJob.submit(payload={\n",
    "        \"node\": 256,\n",
    "        \"machine\":  hpc_machine,\n",
    "        \"walltime\" : 5,\n",
    "        \"file_manager_rel_path\": \"settings/file_manager_constant_airpres.txt\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if lhs_config_prob==1:\n",
    "    communitySummaJob.events(liveOutput=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When outputs are very large, sometimes missing data happens. In that case, try to execute the above cell again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create output folder, and then download the file containing KGE error on outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if lhs_config_prob==1:\n",
    "    job_dir = os.path.join(model_folder, \"{}\".format(communitySummaJob.id))\n",
    "    !mkdir -p {job_dir}/output\n",
    "    communitySummaJob.download(job_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {job_dir} && unzip -o *.zip -d output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_path = os.path.join(job_dir, \"output/regress_data/error_data_configs_latin.nc\")\n",
    "print(error_path)\n",
    "xr.open_dataset(error_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p ./regress_data\n",
    "! cp {error_path} ./regress_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Finish HPC use  </b> (config_latin) config_latin SUMMA runs with each forcing </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2_8 Compute KGE error on outputs\n",
    "We calculate KGE statistics on the data. KGE means perfect agreement if it is 1, and <0 means the mean is a better guess. We use a modified KGE that avoids the amplified simulated mean divided by truth mean values when is the truth mean is small, and avoids the dependence of the KGE metric on the units of measurement. Then, we scale the KGE so that the range is 1 to -1.\n",
    "If the values are identical we use KGE of 1.\n",
    "We also keep summaries of the raw data (summed over time). \n",
    "This can take some time depending on how big of a problem you ran. It takes about 1/100th of the time it took to run the whole problem. \n",
    "\n",
    "\n",
    "All KGE codes were moved over to the CyberGIS-Compute Service and will execute on HPC. The codes can be found here.\n",
    "\n",
    "[https://github.com/cybergis/Jupyter-xsede/blob/master/cybergis/summa.py#L268](https://github.com/cybergis/Jupyter-xsede/blob/master/cybergis/summa.py#L268)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the time spent running the entire notebook.\n",
    "# records the time at this instant \n",
    "# of the program\n",
    "runtime_end = timeit.default_timer()\n",
    "\n",
    "# printing the execution time by subtracting\n",
    "# the time before the function from\n",
    "# the time after the function\n",
    "print(int(runtime_end-runtime_start), ' seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SUMMA-2021-09",
   "language": "python",
   "name": "pysumma-2021-09"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
