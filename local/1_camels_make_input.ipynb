{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "# records the time at this instant\n",
    "# of the program\n",
    "runtime_start = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Creating input files for pysumma\n",
    "\n",
    "In this notebook, we set the study basins, and simulation period and put together forcing data setups for CAMELS.\n",
    "This requires hourly forcings of\n",
    " - temperature (K)\n",
    " - precipitation (kg/m^2/s^1)\n",
    " - shortwave radiation (W/m^2)\n",
    " - longwave radiation (W/m^2)\n",
    " - specific humidity (g/g)\n",
    " - air pressure (Pa)\n",
    " - wind speed (m/s)\n",
    " \n",
    "NLDAS hourly forcings are considered to be \"truth\". \n",
    "We take NLDAS hourly forcings and give them their mean daily values for each variable in turn (or all variables). \n",
    "Then, we take NLDAS daily forcings of maximum and minimum temperature, total precipitation, and mean windspeed; and redistribute them to hourly with calculation of the other variables using the MetSim python package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1_1 Preliminary steps\n",
    "### 1_1_1 Check the enviroment\n",
    "Check that we loaded correct environment. This should show pysumma version 3.0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda list summa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1_1_2 Import libraries\n",
    "Import the required librarires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import geoviews as gv\n",
    "import geopandas as gpd\n",
    "import holoviews as hv\n",
    "import xarray as xr\n",
    "import ogr\n",
    "import cartopy\n",
    "import os\n",
    "import ipyplot\n",
    "\n",
    "hv.notebook_extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1_2 Set up paths to SUMMA configuration files for CAMELS basins\n",
    "### 1_2_1 Unzip the folder contatining SUMMA CAMELS configurations\n",
    "\n",
    "You will need to unzip `summa_camels_hydroshare.zip` data if it is not already unzipped. This folder contains the SUMMA configuration, local attributes, and parameters files that were created using CAMELS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: To be more organized, unzip in a folder inside the contents instead of unzipping at the content level with no folder!\n",
    "if not os.path.exists(\"summa_camels\"):\n",
    "    !unzip summa_camels_hydroshare.zip\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1_2_2 Set up paths to SUMMA configuration files\n",
    "Set up the paths to summa_camels_hydroshare subfolders for the simulations in this notebook and next ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_folder = os.path.join(os.getcwd(), 'summa_camels')\n",
    "settings_folder = os.path.join(top_folder, 'settings')\n",
    "# Shapefile containing the 671 CAMELS basins with their attributes (e.g., hru_id)\n",
    "shapefile = os.path.join(os.getcwd(), 'basin_set_full_res_simple/HCDN_nhru_final_671.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1_3 Select basins and simlaution period\n",
    "\n",
    "We will use a setup which uses CAMELS basin as lumped, so considered as one \"hydrologic response unit\" or HRU.\n",
    "A HRU typically delineates a watershed by topography, soil type, land use, or other defining feature.\n",
    "There are a possible 671 CAMELS basins to use; below you can choose the ID. Because this is an unstructured mesh we will be defining the mesh elements by their respective basin identifiers.\n",
    "\n",
    "If you haven't done already, download NLDAS forcing data from HydroShare OpenDAP (http://hyrax.hydroshare.org/opendap/hyrax/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1_3_1 Retrieve the meteorological forcings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Hydroshare THREDDS Service retrieve the nldas Forcing (1980 to 2018) from the first HydroShare resource: a28685d2dd584fe5885fc368cb76ff2\n",
    "extra_vars0 = xr.open_dataset('http://thredds.hydroshare.org/thredds/dodsC/hydroshare/resources/a28685d2dd584fe5885fc368cb76ff2a/data/contents/nldasForcing1980to2018.nc')\n",
    "# TODO: confrim what the next line does: set time step as hourly? (3600 s =1 hour?!) also, it is already hourly why do we use 3600?\n",
    "extra_vars0['data_step'] = 3600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1_3_2 Select basins and simulation period\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b> Select simulation period and basins:  </b> You will select a time period out of this to run the simulations. You cannot run fewer basins though, so choose wisely as every basin added means more computational expense.   </div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select start and end dates for simulations\n",
    "start_date = '1990-09-30'\n",
    "end_date = '1996-09-30'\n",
    "\n",
    "# All HRUs, select out all or some to look at\n",
    "#the_hru = np.array(extra_vars0['hruId']) #run all\n",
    "hru_ids = [1632900] #HRU_ID: 1632900 is pre-populated, other ones from the paper are 2212600, 9378630, 11264500\n",
    "# hru_ids = [1632900, 2212600, 9378630, 11264500]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1_3_3 Slice the forcings to selected basins and simulaiotn period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_hru = np.array(hru_ids)\n",
    "\n",
    "# In this study, we assume we have GRUs (grouped response units) same as HRUs while GRUs can be made up of one or more HRU\n",
    "the_gru = the_hru\n",
    "\n",
    "# Slice to seleced HRUs\n",
    "extra_vars1 = extra_vars0.sel(hru=the_hru)\n",
    "\n",
    "# Slice to selected time period\n",
    "extra_vars = extra_vars1.loc[dict(time=slice(start_date, end_date))] \n",
    "print(the_hru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1_3_4 Slice the SUMMA CAMELS shapefile to selected basins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(shapefile)\n",
    "shapes = cartopy.io.shapereader.Reader(shapefile)\n",
    "list(shapes.records())[gdf[gdf['hru_id']==hru_ids[0]].index.values[0]] #just to see what an entry looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data from an xarray dataset to a pandas dataframe \n",
    "out_df = extra_vars0['hru']\n",
    "out_df = out_df.to_dataframe()\n",
    "# Make sure we have some metadata to join with the shapefile\n",
    "out_df['hru_id'] = gdf['hru_id'].values\n",
    "#search for the ones with desired records\n",
    "find_rec = out_df.loc[the_hru,:]['hru_id']\n",
    "#look at attributes\n",
    "desired_shapes = []\n",
    "for i in find_rec:\n",
    "    for s in shapes.records():\n",
    "        if s.attributes['hru_id'] == i :\n",
    "            desired_shapes.append(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1_3_5 Show the selected basins in map\n",
    "Once the next cell is ran successfully, use the tools on top right of the map to interact with the map. \n",
    "\n",
    "Basins are colored by index values and are hoverable for index values and IDs. Selected basins are higlighted in cyan. You can look up other basin IDs on this interactive map if you'd like to explore them instead (changing the selection above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create backgound\n",
    "mapp =gv.tile_sources.StamenTerrainRetina.opts(width=900, height=500)\n",
    "# Create the shape plot\n",
    "poly = gv.Shape.from_records(shapes.records(), out_df, index=['hru_id'], on='hru_id',crs=cartopy.crs.PlateCarree())\n",
    "poly2 = gv.Shape.from_records(desired_shapes,out_df.loc[the_hru,:],on='hru_id', crs=cartopy.crs.PlateCarree())\n",
    "poly = poly.opts(cmap='plasma', tools=['hover'], colorbar=True, alpha=0.8)\n",
    "poly2 = poly2.opts(fill_color='cyan', line_color='cyan', alpha=0.8)\n",
    "# Plot\n",
    "mapp*poly*poly2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using 10 or fewer basins, the cell below will plot each basin in detail. Multiple HRUs per basin will be shown in red; we are not using that distributed setup but instead combining the HRUs into a single lumped HRU basin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(the_hru)<=10:\n",
    "    for i in the_hru:\n",
    "        if len(str(i)) <=7:\n",
    "            images_array1 = ['https://staff.ral.ucar.edu/wood/watersheds/dem_figs/0'+str(i)+'.dem.png', 'https://staff.ral.ucar.edu/wood/watersheds/basin_figs/0'+str(i)+'.watershed.png']\n",
    "        else:\n",
    "            images_array1 = ['https://staff.ral.ucar.edu/wood/watersheds/dem_figs/'+str(i)+'.dem.png', 'https://staff.ral.ucar.edu/wood/watersheds/basin_figs/'+str(i)+'.watershed.png']\n",
    "        ipyplot.plot_images(images_array1, max_images=20, img_width=400)\n",
    "the_hru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1_3_6 Slice the SUMMA CAMELS paramters and attributes files to the selected basins \n",
    "We have to select out only the HRU IDs of the basins we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attributes \n",
    "attrib = xr.open_dataset(settings_folder+'/attributes.camels.v2.nc')\n",
    "attrib = attrib.assign_coords(hru=attrib['hruId'])\n",
    "attrib = attrib.assign_coords(gru=attrib['gruId'])\n",
    "gg = attrib['gruId'] # save because gruId was missing from the parameter file\n",
    "attrib = attrib.sel(hru=the_hru)\n",
    "attrib = attrib.sel(gru=the_gru)\n",
    "attrib = attrib.drop(['hru','gru']) #summa doesn't like these to have coordinates\n",
    "attrib.to_netcdf(settings_folder+'/attributes.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "param = xr.open_dataset(settings_folder+'/trialParams.camels.Oct2020.nc')\n",
    "param = param.assign_coords(hru=param['hruId'])\n",
    "param = param.assign_coords(gru=gg) # there should be a gruId in here, but there wasn't\n",
    "param = param.sel(hru=the_hru)\n",
    "param = param.sel(gru=the_gru)\n",
    "param = param.drop(['hru','gru']) #summa doesn't like these to have coordinates\n",
    "param.to_netcdf(settings_folder+'/parameters.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1_3_7 Make the constant initial conditions\n",
    "Lastly, we will need to make the constant initial conditions file, for 8 constant soil layers and 0 snow layers at the start of the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {settings_folder}; source activate /opt/conda/envs/pysumma; python gen_coldstate.py attributes.nc init_cond.nc int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1_4 Create forcing files with constant daily values at their daily means\n",
    "Here we make the NLDAS data hourly values into constant 24 hourly values for each forcing variable. \n",
    "We need to make these 24 hours represent a local day, so local time zones, such that later calculations on days work. We will shift the days so that the time zones line up, make constant, and shift back. As a schematic:\n",
    "\n",
    "![](constantDay.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1_4_1 Write and save the truth forcing\n",
    "Write this file for pysumma forcing and save the forcing file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "truth = extra_vars\n",
    "t0 = truth['time'].values[0] \n",
    "tl = truth['time'].values[-1]\n",
    "t0_s = pd.to_datetime(str(t0))\n",
    "t0_sf =t0_s.strftime('%Y%m%d')\n",
    "tl_s = pd.to_datetime(str(tl))\n",
    "tl_sf =tl_s.strftime('%Y%m%d')\n",
    "ffname ='NLDAStruth_' + t0_sf +'-' + tl_sf +'.nc'\n",
    "truth.to_netcdf(top_folder+'/data/truth/'+ffname)\n",
    "fflistname = settings_folder+'/forcingFileList.truth.txt' \n",
    "file =open(fflistname,\"w\")\n",
    "file.write(ffname)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1_4_2 Shifting to local time zones using longitude values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "To make the daily data, first need to shift to local time zones with longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out variables with no time dimension and add offset\n",
    "ds = truth\n",
    "ds_withtime = ds.drop_vars([ var for var in ds.variables if not 'time' in ds[var].dims ])\n",
    "ds_timeless = ds.drop_vars([ var for var in ds.variables if     'time' in ds[var].dims ])\n",
    "DEG_PER_REV = 360.0       # Number of degrees in full revolution\n",
    "HRS_PER_DAY = 24\n",
    "offset = (attrib['longitude'] / DEG_PER_REV) * HRS_PER_DAY\n",
    "offset = offset.astype(int)\n",
    "ds_withtime['offset'] = offset\n",
    "ds_withtime = ds_withtime.assign_coords(hru=ds_timeless['hruId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "In the next cell, the time zone changes. \n",
    "This takes about a minute using all 671 basins; a subset of basins should be shorter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for t in np.unique(offset.data):\n",
    "    ds = ds_withtime.where(offset==t,drop=True)\n",
    "    ds = ds.shift(time=t)\n",
    "    if t==np.unique(offset.data)[0]: ds0 = ds\n",
    "    if t>np.unique(offset.data)[0]: ds0 = xr.concat([ds0,ds],dim='hru')\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort back, and drop offset\n",
    "ds_withtime = ds0.sortby('hru')\n",
    "ds_withtime = ds_withtime.drop_vars('offset') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1_4_3 Downsample hourly time-series data to daily data\n",
    "Downsample hourly time-series data to daily data. \n",
    "This takes about a 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "truth24 = xr.merge([ds_timeless, ds_withtime.resample(time='1D').mean()]).load()\n",
    "# Fix time encoding to be the same since the merge drops it\n",
    "truth24.time.encoding = extra_vars.time.encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1_4_4 Upsample back to hourly data and undo time zone changes\n",
    "Then we upsample this back to hourly data for constant daily values. \n",
    "We need to undo the time zone changes after we upsample. \n",
    "This whole process takes about a minute using all 671 basins; a subset of basins should be shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a fake day of data so upsamples until the end\n",
    "day_fake = truth24.isel(time=-1)['time']+np.timedelta64(1,'D')\n",
    "add_fake = truth24.isel(time=-1)\n",
    "add_fake['time'] = day_fake\n",
    "truth24_add = xr.concat([truth24, add_fake], dim='time',data_vars='minimal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Again we have to separate out variables with no time dimension.\n",
    "ds = truth24_add\n",
    "ds_withtime = ds.drop_vars([ var for var in ds.variables if not 'time' in ds[var].dims ])\n",
    "ds_timeless = ds.drop_vars([ var for var in ds.variables if     'time' in ds[var].dims ])\n",
    "ds_withtime = ds_withtime.resample(time='1H').ffill()\n",
    "ds_withtime['offset'] = offset\n",
    "ds_withtime = ds_withtime.assign_coords(hru=ds_timeless['hruId'])\n",
    "for t in np.unique(offset.data):\n",
    "    ds = ds_withtime.where(offset==t,drop=True)\n",
    "    ds = ds.shift(time=-t)\n",
    "    if t==np.unique(offset.data)[0]: ds0 = ds\n",
    "    if t>np.unique(offset.data)[0]: ds0 = xr.concat([ds0,ds],dim='hru')\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort back, and drop offset, and merge\n",
    "ds_withtime = ds0.sortby('hru')\n",
    "ds_withtime = ds_withtime.drop_vars('offset') \n",
    "constant_all = xr.merge([ds_timeless, ds_withtime])\n",
    "constant_all = constant_all.sel(hru=the_hru) #put back in original order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take extra day off\n",
    "constant_all = constant_all.isel(time=slice(0,-1))\n",
    "# Fix time encoding to be the same since the merge drops it\n",
    "constant_all.time.encoding = extra_vars.time.encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1_4_5 Scale constant SW radiation\n",
    "We must correct shortwave radiation since SUMMA will impose that it is 0 when the sun is below the horizon. So, we distribute the constant shortwave radiation only during the daylight hours, using the hours NLDAS is 0 as night. Then we increase the magnitude of the shortwave radiation so that the dailys sum of the constant value with the 0 at night is the original. As a schematic:\n",
    "\n",
    "![](constantSWR.jpg)\n",
    "\n",
    "Other changes inside SUMMA are that specific humidity will be lowered in order that relative humidity does not exceed 100%, and tiny windspeeds will be eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find where 0's shoud be based on original NLDAS data\n",
    "zero_one = truth['SWRadAtm']/truth['SWRadAtm']\n",
    "zero_one = zero_one.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find how much too small shortwave is each day if we use these 0's\n",
    "swr0 = zero_one*constant_all['SWRadAtm']\n",
    "div = swr0.resample(time='1D').mean()/constant_all['SWRadAtm'].resample(time='1D').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample, again add a fake day of data so upsamples until the end\n",
    "add_fake = div.isel(time=-1)\n",
    "add_fake['time'] = day_fake\n",
    "div_add = xr.concat([div, add_fake], dim='time')\n",
    "div_add = div_add.resample(time='1H').ffill()\n",
    "\n",
    "# Take extra day off\n",
    "div = div_add.isel(time=slice(0,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally add back in this constant shortwave radiation\n",
    "swr0 = swr0/div\n",
    "constant_all['SWRadAtm']=swr0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1_4_6 Create files with only one variable constant at their daily values\n",
    "Now make files with only one variable held at daily means and save forcing file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = constant_all['time'].values[0] \n",
    "tl = constant_all['time'].values[-1]\n",
    "t0_s = pd.to_datetime(str(t0))\n",
    "t0_sf =t0_s.strftime('%Y%m%d')\n",
    "tl_s = pd.to_datetime(str(tl))\n",
    "tl_sf =tl_s.strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "constant_vars=['airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd']\n",
    "for v in constant_vars:\n",
    "    constant_one = truth.copy()\n",
    "    constant_one[v]= constant_all[v]\n",
    "    ffname ='NLDASconstant_' + v +'_forcing_' + t0_sf +'-' + tl_sf +'.nc'\n",
    "    constant_one.to_netcdf(top_folder+'/data/constant/'+ffname)\n",
    "    fflistname = settings_folder+'/forcingFileList.constant_' + v + '.txt' \n",
    "    file =open(fflistname,\"w\")\n",
    "    file.write(ffname)\n",
    "    file.close()\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1_5 Check processed forcing files through plotting\n",
    "\n",
    "To make sure things look how we want, we plot the constant dataset against the NLDAS \"truth\" dataset, and plot the cumulative variables to see how errors are compounding. \n",
    "We plot one HRU (the first one) for 2 months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1_5_1 Hourly plots of the forcings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot hourly\n",
    "fig, axes = plt.subplots(nrows=7, ncols=1, figsize=(20, 20))\n",
    "axes = axes.flatten()\n",
    "axes[0].set_title('Hourly')\n",
    "\n",
    "unit_str = ['($ ^o K$)', '($kg/m/s$)', '($W/m^2$)','($w/m^2$)','($g/g$)','($Pa$)', '($m/s$)',]\n",
    "\n",
    "#start =  24*9*30 #summer\n",
    "start =  24*15*30 #winter\n",
    "stop = start + 2*30*24 \n",
    "truth_plt = truth.isel(hru=0, time=slice(start, stop))\n",
    "constant_all_plt = constant_all.isel(hru=0, time=slice(start, stop))\n",
    "\n",
    "for idx, var in enumerate(constant_vars):\n",
    "    truth_plt[var].plot(ax=axes[idx],label='NLDAS')\n",
    "    constant_all_plt[var].plot(ax=axes[idx],label='Constant')\n",
    "    axes[idx].set_title('') \n",
    "    axes[idx].set_ylabel('{} {}'.format(var, unit_str[idx]), fontsize=18)\n",
    "    axes[idx].set_xlabel('Date',fontsize=18)\n",
    "    axes[idx].tick_params(axis='both', which='major', labelsize=13)\n",
    "plt.tight_layout()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1_5_2 Cummulative plots of the forcings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot cummulative\n",
    "fig, axes = plt.subplots(nrows=7, ncols=1, figsize=(20, 20))\n",
    "axes = axes.flatten()\n",
    "axes[0].set_title('Cumulative')\n",
    "\n",
    "truth_plt = truth.isel(hru=0, time=slice(start, stop)).cumsum(dim='time')\n",
    "constant_all_plt = constant_all.isel(hru=0, time=slice(start, stop)).cumsum(dim='time')\n",
    "\n",
    "for idx, var in enumerate(constant_vars):\n",
    "    truth_plt[var].plot(ax=axes[idx],label='NLDAS')\n",
    "    constant_all_plt[var].plot(ax=axes[idx],label='Constant')\n",
    "    axes[idx].set_title('') \n",
    "    axes[idx].set_ylabel('{} {}'.format(var, unit_str[idx]), fontsize=18)\n",
    "    axes[idx].set_xlabel('Date', fontsize=18)\n",
    "    axes[idx].tick_params(axis='both', which='major', labelsize=13)\n",
    "plt.tight_layout()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines, if you need to clean up your directory. Note that if you do so, the rest of notebooks will not run as notebook 1 outputs are inputs for them.\n",
    "# To avoid denied permission when deleting files, as some files are used by other notebooks, first shutdown the kernels for all notebooks, then set the kernel for this notebook,\n",
    "# run, this block, and once the folders are removed, comment this block again.\n",
    "\n",
    "# import os, shutil\n",
    "# CURRENT_PATH = os.getcwd()\n",
    "# CURRENT_PATH\n",
    "\n",
    "# path = '/home/jovyan/work/Downloads/50e9716922dc487981b71e2e11f3bb5d/50e9716922dc487981b71e2e11f3bb5d/data/contents/dask-worker-space'\n",
    "# !rm -rf {path}\n",
    "\n",
    "# path = '/home/jovyan/work/Downloads/50e9716922dc487981b71e2e11f3bb5d/50e9716922dc487981b71e2e11f3bb5d/data/contents/summa_camels'\n",
    "# !rm -rf {path}\n",
    "\n",
    "# path = '/home/jovyan/work/Downloads/50e9716922dc487981b71e2e11f3bb5d/50e9716922dc487981b71e2e11f3bb5d/data/contents/regress_data'\n",
    "# !rm -rf {path}\n",
    "\n",
    "# path = '/home/jovyan/work/Downloads/50e9716922dc487981b71e2e11f3bb5d/50e9716922dc487981b71e2e11f3bb5d/data/contents/basin_set_full_res_simple'\n",
    "# !rm -rf {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the time spent running the entire notebook.\n",
    "# records the time at this instant \n",
    "# of the program\n",
    "runtime_end = timeit.default_timer()\n",
    "\n",
    "# printing the execution time by subtracting\n",
    "# the time before the function from\n",
    "# the time after the function\n",
    "print(int(runtime_end-runtime_start), ' seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SUMMA-2021-09",
   "language": "python",
   "name": "pysumma-2021-09"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
