{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the SUMMA setups\n",
    "Here, we run SUMMA using pysumma, on the setups we created in the previous notebook.\n",
    "This is a computationally expensive notebook. \n",
    "If you don't have the computer power or time, you can run a less complex problem than the full problem, but the output (and graphs) in the next notebook will not be complete. \n",
    "If things start to fail, restart the server and run the simuluations in smaller loops before restarts. \n",
    "The last section of the notebook computes summary statistics of the output to be used in the next notebook.\n",
    "\n",
    "Complexity choices are, in order of increasing complexity: \n",
    " - 1)   `default_prob = 1`: the \"default\" configuration with the \"default\" parameters. By \"default\" we mean whatever you chose in the summa setup files. \n",
    " - 2a) `lhs_prob = 1`: the default configuration with exploration of the parameter space.\n",
    " - 2b) `config_prob = 1`: the default parameters with 8 different configurations (choices that have been seen to affect the model output in previous research) \n",
    " - 3)   `lhs_config_prob = 1`: 8 different configurations with the default and the exploration of the parameter space.\n",
    "\n",
    "The default problem will be run with every complexity choice.\n",
    "\n",
    "Eight iterations of each loop are run for each problem, to cover a truth run and the 7 forcings each held to a daily constant in turn.\n",
    "The time unit in the commnets given is written as \"minoot\", where 1 minoot= 1 minute on the CyberGIS for Water Xcede, but will be different on your computer or another computer. \n",
    "For comparision, one iteration of each problem (in order of increasing complexity) takes: \n",
    " - 1)   8X ~0.3 minoots for 1 HRUs\n",
    " - 2a)  8X ~1.2 minoots for 1 HRU\n",
    " - 2b)  8X ~0.8 minoots for 1 HRU\n",
    " - 3)   8X ~73 minoots for 1 HRU\n",
    " \n",
    "There are times in \"minoots\" for various cells in the notebook, so you can have an idea how long a cell might take to run. \n",
    " \n",
    "You can also make the problem run for fewer years to lower computational costs, changing `str(the_start)` and `str(the_end)`. You can test with as little as a day between `the_start` and `the_end`, but if you want to make the plots in this notebook and run the next notebook you will need >1 year (1 year is considered intialization period). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Make problem complexity choices here:\n",
    " \n",
    "Note, you can choose all of these to be 1, but each step of complexity will contain the previous problem(s), so it is not necessary. It will result in more files, but if may be useful to check that each step of the problem expansion runs successfullly. If you have more than 10 HRUs (CAMELS subbasins in the example problem), it was decided that the problem is too big and you can only run the default problem (the complexity choice will get reset after the number of HRUs is determined). However, theoretically it will run with more HRUs. \n",
    "\n",
    "Another complexity choice you can make is to run a different length time-period. It is pre-populated to run 18 months, as the 6 years in the paper takes a long time without high-performance computing. You also need to choose how long the initialization period is for the error calculations. We suggest 183 to 365 days, with at least 1 more year of simulation. So for example, if you run 18 months of simulation you should choose your initialization to be 183 days.\n",
    "\n",
    "Lastly, if you have any errors while running the pysumma problems, you can run a short time-period (1 day) in simpler pysumma mode by changing `debug` to 1 and rerunning. This will provide more detailed error messages. The problem WILL NOT run for the full time period if `debug = 1`. The initialization period for error calculation will be set to 0 days (but the plots based on these error calculations in the next notebook would be uninformative). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prob = 0\n",
    "lhs_prob = 0\n",
    "config_prob = 0\n",
    "lhs_config_prob = 1 #pre-populated this is 1 and the rest 0, to run full problem that is in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_start = '1991-04-01 00:00' #pre-populated to '1991-04-01 00:00', but '1990-10-01 00:00' is in the paper.\n",
    "the_end =   '1992-09-30 23:00' #pre-populated to '1992-09-30 23:00, but '1996-09-30 23:00' is in the paper.\n",
    "initialization_days = 183 #pre-populated to 183 days, or 6 months, paper uses 365 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 1, ONLY the one day problem with more detailed error messages will not run. \n",
    "# if 0, the problem will run from the_start to the_end as defined above.\n",
    "debug = 0 #pre-populated to 0\n",
    "the_end_debug = the_start[0:11] + '23:00'\n",
    "if debug==1: initialization_days = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Check that we loaded correct environment. This should show pysumma version 3.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda list summa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Load the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pysumma as ps\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Set up the paths and regionalize the paths in the configuration files that SUMMA will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_folder = os.path.join(os.getcwd(), 'summa_camels')\n",
    "settings_folder = os.path.join(top_folder, 'settings')\n",
    "ps_working = os.path.join(top_folder, '.pysumma')\n",
    "regress_folder = os.path.join(os.getcwd(), 'regress_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd {top_folder}; chmod +x installTestCases_local.sh; ./installTestCases_local.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of HRUs\n",
    "attrib = xr.open_dataset(settings_folder+'/attributes.nc')\n",
    "the_hru = np.array(attrib['hruId'])\n",
    "# change complexity if necessary\n",
    "if len(the_hru) >10:\n",
    "    default_prob = 1    \n",
    "    lhs_prob = 0\n",
    "    config_prob = 0\n",
    "    lhs_config_prob = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Interacting with SUMMA via the `Distributed` object\n",
    "\n",
    "If we have more than one basin, we are running a `Distributed` object, which has multiple `Simulation` objects inside, each corresponding to some spatial chunk. (If we only have one basin, we cannot run this way and then must run as a single `Simulation` object.) \n",
    "We need to do `rm -r {ps_working}` to clear out the distributed folders every run so permissions do not get screwed up in the loops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fewer basins, do not exceed number of basins in chunking for Default problem\n",
    "CHUNK = 8 #for all 671 basins or more than 8\n",
    "CHUNK0 = CHUNK\n",
    "if len(the_hru) <8: CHUNK0 = len(the_hru)\n",
    "NCORES=6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "To set up a `Distributed` object you must supply several pieces of information. \n",
    "First, supply the SUMMA executable; this could be either the compiled executable on your local machine, or a docker image. \n",
    "The second piece of information is the path to the file manager, which we just created through the install script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable =  '/usr/bin/summa.exe'  #     \n",
    "file_manager = settings_folder+'/file_manager_truth.txt'\n",
    "if CHUNK0>1: camels = ps.Distributed(executable, file_manager, num_workers=NCORES, chunk_size=CHUNK0)\n",
    "if CHUNK0==1: camels = ps.Simulation(executable, file_manager)\n",
    "camels.manager['simStartTime'] = str(the_start)\n",
    "camels.manager['simEndTime'] = str(the_end)\n",
    "camels.manager.write()\n",
    "print(camels.manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Run Problem with Truth Forcing\n",
    "\n",
    "We run pysumma with NLDAS on each basin, the 'truth run'. You can check how long it has been running by using the command `qstat -u <username>` in a terminal. This takes about 0.3 minoots to run with 1 HRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# truth, default parameters and configuration\n",
    "if default_prob==1 and debug==0:\n",
    "    camels.run('local')\n",
    "    if CHUNK0>1:\n",
    "        all_status = [(n, s.status) for n, s in camels.simulations.items()] #if want to look at status if has errors\n",
    "        all_ds = [s.output.load() for n, s in camels.simulations.items()] #load it into memory so faster\n",
    "    if CHUNK0==1:\n",
    "        all_status = camels.status #if want to look at status if has errors\n",
    "        all_ds = camels.output.load() #load it into memory so faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "We will merge the GRU and HRU files if we ran the distibuted. We could just write it as several files instead of merging. However, if we want to merge, we can do the following.\n",
    "First, detect automatically which vars have hru vs gru dimensions (depending on what we use for output, we may not have any gru):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if default_prob==1 and debug==0:\n",
    "    hru_vars = [] # variables that have hru dimension\n",
    "    gru_vars = [] # variables that have gru dimension\n",
    "    if CHUNK0>1:\n",
    "        for ds in all_ds:\n",
    "            for name, var in ds.variables.items():\n",
    "                if 'hru' in var.dims:\n",
    "                    hru_vars.append(name)\n",
    "                elif 'gru' in var.dims:\n",
    "                    gru_vars.append(name)\n",
    "    if CHUNK0==1:\n",
    "        for name, var in all_ds.variables.items():\n",
    "            if 'hru' in var.dims:\n",
    "                hru_vars.append(name)\n",
    "            elif 'gru' in var.dims:\n",
    "                gru_vars.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "If we ran distributed, filter variables for merge, write merged files, and delete stuff to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if default_prob==1 and debug==0: \n",
    "    if CHUNK0>1:\n",
    "        hru_ds = [ds[hru_vars] for ds in all_ds]\n",
    "        gru_ds = [ds[gru_vars] for ds in all_ds]\n",
    "        hru_merged = xr.concat(hru_ds, dim='hru')\n",
    "        gru_merged = xr.concat(gru_ds, dim='gru')\n",
    "    if CHUNK0==1:\n",
    "        hru_merged = all_ds[hru_vars]\n",
    "        gru_merged = all_ds[gru_vars]\n",
    "    print(hru_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if default_prob==1 and debug==0:\n",
    "    hru_merged.to_netcdf(top_folder+'/output/merged_day/NLDAStruth_hru.nc')\n",
    "    gru_merged.to_netcdf(top_folder+'/output/merged_day/NLDAStruth_gru.nc')\n",
    "    del camels\n",
    "    del all_ds \n",
    "    del hru_merged\n",
    "    del gru_merged\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Run Problem with Constant Forcing\n",
    "\n",
    "Here are the other runs with each forcing held constant, now as a loop. \n",
    "We delete stuff after every run to reduce memory needs. This takes about 7 X 0.3 minoots to run with 1 HRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Each forcing constant, default parameters and configuration\n",
    "if default_prob==1 and debug==0:\n",
    "    constant_vars= ['airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd']\n",
    "    for v in constant_vars:\n",
    "        ! rm -rf {ps_working}\n",
    "        file_manager = settings_folder+'/file_manager_constant_' + v +'.txt'\n",
    "        if CHUNK0>1: camels = ps.Distributed(executable, file_manager, num_workers=NCORES, chunk_size=CHUNK0)\n",
    "        if CHUNK0==1: camels = ps.Simulation(executable, file_manager)\n",
    "        camels.manager['simStartTime'] = str(the_start)\n",
    "        camels.manager['simEndTime'] = str(the_end) \n",
    "        camels.manager.write()        \n",
    "        camels.run('local')\n",
    "        if CHUNK0>1:\n",
    "            all_status = [(n, s.status) for n, s in camels.simulations.items()] #if want to look at status if has errors\n",
    "            all_ds = [s.output.load() for n, s in camels.simulations.items()] #load it into memory so faster\n",
    "        if CHUNK0==1:\n",
    "            all_status = camels.status #if want to look at status if has errors\n",
    "            all_ds = camels.output.load() #load it into memory so faster\n",
    "        hru_vars = [] # variables that have hru dimension\n",
    "        gru_vars = [] # variables that have gru dimension\n",
    "        if CHUNK0>1:\n",
    "            for ds in all_ds:\n",
    "                for name, var in ds.variables.items():\n",
    "                    if 'hru' in var.dims:\n",
    "                        hru_vars.append(name)\n",
    "                    elif 'gru' in var.dims:\n",
    "                        gru_vars.append(name)\n",
    "            hru_ds = [ds[hru_vars] for ds in all_ds]\n",
    "            gru_ds = [ds[gru_vars] for ds in all_ds]\n",
    "            hru_merged = xr.concat(hru_ds, dim='hru')\n",
    "            gru_merged = xr.concat(gru_ds, dim='gru')            \n",
    "        if CHUNK0==1:\n",
    "            for name, var in all_ds.variables.items():\n",
    "                if 'hru' in var.dims:\n",
    "                    hru_vars.append(name)\n",
    "                elif 'gru' in var.dims:\n",
    "                    gru_vars.append(name)\n",
    "            hru_merged = all_ds[hru_vars]\n",
    "            gru_merged = all_ds[gru_vars]\n",
    "        hru_merged.to_netcdf(top_folder+'/output/merged_day/NLDASconstant_' + v +'_hru.nc')\n",
    "        gru_merged.to_netcdf(top_folder+'/output/merged_day/NLDASconstant_' + v +'_gru.nc')\n",
    "        del camels\n",
    "        del all_ds \n",
    "        del hru_merged\n",
    "        del gru_merged\n",
    "        gc.collect()\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "If you have errors, debug mode will run a short time-period in non-distributed (i.e. standard) pysumma mode. Set  `debug = 1` in the complexity choices. This will provide more detailed error messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if default_prob==1 and debug==1:\n",
    "    s = ps.Simulation(executable, file_manager)\n",
    "    s.manager['simStartTime'] = str(the_start)\n",
    "    s.manager['simEndTime'] = str(the_end_debug)\n",
    "    s.run('local', run_suffix='_default')\n",
    "    assert s.status == 'Success'\n",
    "    print(s.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Exploring the Parameter Calibration Space with a Latin Hypercube\n",
    "\n",
    "The above models were run on the default parameter set only. Let's rerun with parameter sets selected by using a Latin Hypercube to get 10 different parameter sets for every HRU, in order to explore the calibration space. This will show us if the results of forcing importance could change after calibration. \n",
    "\n",
    "Currrently, none of the model parameters (or decisions) can be altered in a `Distributed` object. \n",
    "However, if we switch to `Simulation` objects and use the `Ensemble` class, we can run suites of different model parameters with relative ease. \n",
    "\n",
    "We change only the parameters that are usually calibrated. You can remove parameters if you were not planning to ever calibrate them away from their defaults (likewise you could add parameters).\n",
    "\n",
    "The absolute minimums and maximums will break simulations and zero out variables, so we do not use those, we stay at the 5% level away from the extremes. Also, there are some constraints on the parameters that must be followed, they are:\n",
    "\n",
    "* heightCanopyTop   > heightCanopyBottom\n",
    "* critSoilTranspire > theta_res\n",
    "* theta_sat         > critSoilTranspire\n",
    "* fieldCapacity     > theta_res\n",
    "* theta_sat         > fieldCapacity\n",
    "* theta_sat         > theta_res\n",
    "* critSoilTranspire > critSoilWilting\n",
    "* critSoilWilting   > theta_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_prob==1 or lhs_config_prob==1: from pyDOE import lhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    file_manager = settings_folder+'/file_manager_truth.txt'\n",
    "    s = ps.Simulation(executable, file_manager)\n",
    "    s.manager['simStartTime'] = str(the_start)\n",
    "    s.manager['simEndTime'] = str(the_end)  \n",
    "    #Before running the ensemble that changes parameters we must write the original simulation's parameters.\n",
    "    s.manager.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the default, min, and max as in /settings.v1/localParamInfo.txt and /settings.v1/basinParamInfo.txt\n",
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    param_calib_hru = ['albedoRefresh', 'aquiferBaseflowExp', 'aquiferBaseflowRate', 'frozenPrecipMultip', 'heightCanopyBottom','heightCanopyTop', 'k_macropore', \n",
    "                   'k_soil', 'qSurfScale', 'summerLAI', 'tempCritRain', 'theta_sat', 'windReductionParam'] \n",
    "    param_calib_gru = ['routingGammaScale', 'routingGammaShape']\n",
    "\n",
    "    for k in param_calib_hru:\n",
    "        print(s.global_hru_params[k])\n",
    "    for k in param_calib_gru:\n",
    "        print(s.global_gru_params[k]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    bounds_hru = np.full((len(param_calib_hru),3),1.0)\n",
    "    bounds_gru = np.full((len(param_calib_gru),3),1.0)\n",
    "    for i,k in enumerate(param_calib_hru): bounds_hru[i,]= s.global_hru_params.get_value(k)[0:3]\n",
    "    for i,k in enumerate(param_calib_gru): bounds_gru[i,]= s.global_gru_params.get_value(k)[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bounds and expand to size of LHS runs\n",
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    numl = 10\n",
    "    num_vars =  len(param_calib_hru) + len(param_calib_gru)\n",
    "    names =  param_calib_hru + param_calib_gru\n",
    "    bounds =  np.concatenate((bounds_hru, bounds_gru), axis=0)\n",
    "    par_def = dict(zip(names, np.transpose(np.tile(bounds[:,0],(len(the_hru),1))) ))\n",
    "    par_min = dict(zip(names, np.transpose(np.tile(bounds[:,1],(numl*len(the_hru),1))) ))\n",
    "    par_max = dict(zip(names, np.transpose(np.tile(bounds[:,2],(numl*len(the_hru),1))) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "We remove geographically distributed parameters from the default set, and then make the LHS parameter set plus deault from the above bounds. Set 0 will be the default parameter set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove geographically distributed parameters from default set\n",
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    distributed_val = par_def.pop('heightCanopyBottom')\n",
    "    distributed_val = par_def.pop('heightCanopyTop')\n",
    "    distributed_val = par_def.pop('k_soil')\n",
    "    distributed_val = par_def.pop('theta_sat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to obey parameter constraints\n",
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    param = xr.open_dataset(settings_folder+'/parameters.nc')\n",
    "\n",
    "    for i,h in enumerate(the_hru):\n",
    "        lb_theta_sat = max(param[['critSoilTranspire','fieldCapacity','theta_res']].isel(hru=i).values()).values\n",
    "        for j in range(0,numl): #say first numl belong to hru 0, second numl to hru 1, and so on\n",
    "            if (par_min['theta_sat'][j + i*numl]<lb_theta_sat): par_min['theta_sat'][j + i*numl]=lb_theta_sat\n",
    "\n",
    "    par_min['heightCanopyTop'] = par_max['heightCanopyBottom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a 5% buffer\n",
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    buff = {key: (par_max.get(key) - par_min.get(key))*0.05 for key in set(par_max) }\n",
    "    par_min = {key: par_min.get(key) + buff.get(key)*0.05 for key in set(buff) }\n",
    "    par_max = {key: par_max.get(key) - buff.get(key)*0.05 for key in set(buff) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate samples with Latin Hypercube Sampling, set seed by HRU ID so it is the same every time run\n",
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    lhd = np.empty(shape=(num_vars,numl*len(the_hru)))\n",
    "    for i, h in enumerate(the_hru):\n",
    "        np.random.seed(h) #if the hru ID is not a number this will not work\n",
    "        lhd[:,range(i*numl,(i+1)*numl)] = lhs(numl, samples=num_vars)\n",
    "    lhd = dict(zip(names,lhd))\n",
    "    samples = {key: par_min.get(key) + lhd.get(key)*(par_max.get(key) - par_min.get(key)) for key in set(par_max) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make ensembles\n",
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    latin = {}\n",
    "    latin[str(0)] = {'trial_parameters': {key: par_def.get(key) for key in set(par_def) }}\n",
    "    for j in range(0,numl):\n",
    "            latin[str(j+1)] = {'trial_parameters': {key: samples.get(key)[np.arange(j, len(the_hru)*numl, numl)] for key in set(samples) }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now we just do what we did before in the simulations previously, except here we merge with a new dimension of the the configuration decision identifier instead of by `hru` and `gru`. This takes about 1.2 minoots to run with 1 HRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# truth, parameter space with default configuration\n",
    "if lhs_prob==1 and debug==0:\n",
    "    ! rm -rf {ps_working}\n",
    "    param_ens = ps.Ensemble(executable, latin, file_manager, num_workers=NCORES)   \n",
    "    param_ens.run('local')\n",
    "    all_status = [(n, s.status) for n, s in param_ens.simulations.items()] #if want to look at status if has errors\n",
    "    all_ds = [s.output.load() for n, s in param_ens.simulations.items()] #load it into memory so faster  \n",
    "    all_name = [n for n, s in param_ens.simulations.items()]\n",
    "    all_merged = xr.concat(all_ds, pd.Index(all_name, name=\"decision\"))\n",
    "    all_merged.to_netcdf(top_folder+'/output/merged_day/NLDAStruth_latin.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all were a success\n",
    "if lhs_prob==1 and debug==0:\n",
    "    print(all_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "To make sure things look how we want, we plot cumulative variables to see how differences are compounding.\n",
    "We plot one HRU (the first one) for 2 months. We are showing winter months, but this can be switched to summer months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot cummulative\n",
    "if lhs_prob==1 and debug==0:\n",
    "    fig, axes = plt.subplots(nrows=14, ncols=1, figsize=(20, 40))\n",
    "    axes = axes.flatten()\n",
    "    axes[0].set_title('Cumulative')\n",
    "\n",
    "    variables = list(all_merged.variables.keys())[7:22]\n",
    "\n",
    "    #start =  24*3*30 #summer\n",
    "    start =  24*9*30 #winter\n",
    "    stop = start + 2*30*24 \n",
    "    truth_plt = all_merged.isel(hru=0, time=slice(start, stop))\n",
    "\n",
    "    for idx, var in enumerate(variables[0:14]):\n",
    "        for i, dec in enumerate(all_name):    \n",
    "            truth_plt[var].isel(decision=i).plot(ax=axes[idx],label=dec)\n",
    "        axes[idx].set_title('') \n",
    "        axes[idx].set_ylabel(var)\n",
    "        axes[idx].set_xlabel('Date')\n",
    "    plt.tight_layout()\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cummulative\n",
    "if lhs_prob==1 and debug==0:\n",
    "    all_merged = xr.open_dataset(top_folder+'/output/merged_day/NLDAStruth_latin.nc')\n",
    "    fig, axes = plt.subplots(nrows=14, ncols=1, figsize=(20, 40))\n",
    "    axes = axes.flatten()\n",
    "    axes[0].set_title('Cumulative')\n",
    "\n",
    "    variables = list(all_merged.variables.keys())[7:22]\n",
    "\n",
    "    #start =  24*3*30 #summer\n",
    "    start =  24*9*30 #winter\n",
    "    stop = start + 2*30*24 \n",
    "    truth_plt = all_merged.isel(hru=0, time=slice(start, stop))\n",
    "\n",
    "    for idx, var in enumerate(variables[0:14]):\n",
    "        for i, dec in enumerate(all_name):    \n",
    "            truth_plt[var].isel(decision=i).plot(ax=axes[idx],label=dec)\n",
    "        axes[idx].set_title('') \n",
    "        axes[idx].set_ylabel(var)\n",
    "        axes[idx].set_xlabel('Date')\n",
    "    plt.tight_layout()\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete things to save memory as before\n",
    "if lhs_prob==1 and debug==0:\n",
    "    del param_ens\n",
    "    del all_ds \n",
    "    del all_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now run the other forcing sets as a loop. This takes about 7 X 1.2 minoots to run with 1 HRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Each forcing constant, parameter space with default configuration\n",
    "if lhs_prob==1 and debug==0:\n",
    "    constant_vars= ['airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd']\n",
    "    for v in constant_vars:\n",
    "        ! rm -rf {ps_working}\n",
    "        file_manager = settings_folder+'/file_manager_constant_' + v +'.txt'\n",
    "        s = ps.Simulation(executable, file_manager)\n",
    "        s.manager['simStartTime'] = str(the_start)\n",
    "        s.manager['simEndTime'] = str(the_end)  \n",
    "        s.manager.write()        \n",
    "        param_ens = ps.Ensemble(executable, latin, file_manager, num_workers=NCORES)   \n",
    "        param_ens.run('local')\n",
    "        all_status = [(n, s.status) for n, s in param_ens.simulations.items()] #if want to look at status if has errors\n",
    "        all_ds = [s.output.load() for n, s in param_ens.simulations.items()] #load it into memory so faster  \n",
    "        all_name = [n for n, s in param_ens.simulations.items()]\n",
    "        all_merged = xr.concat(all_ds, pd.Index(all_name, name=\"decision\"))\n",
    "        all_merged.to_netcdf(top_folder+'/output/merged_day/NLDASconstant_' + v +'_latin.nc')\n",
    "        del param_ens\n",
    "        del all_ds \n",
    "        del all_merged\n",
    "        gc.collect()\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "If you have errors, debug mode will run a short time-period in non-ensemble (i.e. standard) pysumma mode. Set  `debug = 1` in the complexity choices. This will provide more detailed error messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_prob==1 and debug==1:\n",
    "    s = ps.Simulation(executable, file_manager)\n",
    "    s.manager['simStartTime'] = str(the_start)\n",
    "    s.manager['simEndTime'] = str(the_end_debug)\n",
    "    s.run('local', run_suffix='_default')\n",
    "    assert s.status == 'Success'\n",
    "    print(0, s.status) #0 is default parameter set\n",
    "    for j in range(0,numl):\n",
    "        s = ps.Simulation(executable, file_manager)\n",
    "        s.manager['simStartTime'] = str(the_start)\n",
    "        s.manager['simEndTime'] = str(the_end_debug)\n",
    "        for key in set(samples):\n",
    "            s.trial_params[key].values =  samples.get(key)[np.arange(j, len(the_hru)*numl, numl)]\n",
    "        s.run('local', run_suffix='_default')\n",
    "        assert s.status == 'Success'\n",
    "        print(j+1, s.status) #j+1 is the number of the LHS parameter set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Manipulating the Configuration of the pysumma Objects\n",
    "\n",
    "We need to run the parameter space with other model configurations, to see if the results seen on the default configuration hold true across the parameter space. The new configurations follow the exploration of [this paper.](https://doi.org/10.1002/2015WR017200).\n",
    "\n",
    "Clark, M.P., Nijssen, B., Lundquist, J.D., Kavetski, D., Rupp, D.E., Woods, R.A., Freer, J.E., Gutmann, E.D., Wood, A.W., Gochis, D.J. and Rasmussen, R.M., 2015. A unified approach for process‐based hydrologic modeling: 2. Model implementation and case studies. Water Resources Research, 51(4), pp.2515-2542.\n",
    "\n",
    "Of the model configurations discussed in this paper, the decisions that made the most difference are:\n",
    "\n",
    " - `groundwatr` choice of groundwater parameterization as:\n",
    "   - `qTopmodl` the topmodel parameterization (note must set hc_profile = pow_prof and bcLowrSoiH = zeroFlux\n",
    "   - `bigBuckt` a big bucket (lumped aquifer model) in between the other two choices for complexity\n",
    "   - `noXplict` no explicit groundwater parameterization\n",
    " - `stomResist` choice of function for stomatal resistance as:\n",
    "   - `BallBerry` Ball-Berry (1987) parameterization of physiological factors controlling transpiration\n",
    "   - `Jarvis` Jarvis (1976) parameterization of physiological factors controlling transpiration\n",
    "   - `simpleResistance` parameterized solely as a function of soil moisture limitations\n",
    " - `snowIncept` choice of parameterization for snow interception as:\n",
    "   - `stickySnow` maximum interception capacity is an increasing function of temperature\n",
    "   - `lightSnow` maximum interception capacity is an inverse function of new snow density\n",
    " - `windPrfile` choice of wind profile as:\n",
    "   - `exponential` an exponential wind profile extends to the surface\n",
    "   - `logBelowCanopy` a logarithmic profile below the vegetation canopy\n",
    "\n",
    "Choices `bigBuckt`, `BallBerry`, `lightSnow`, and `logBelowCanopy` are the defaults that we have run already (see the decisions printed out in the previous cell). The paper showed choice of `groundwatr` affecting the timing of runoff and the magnitude of evapotranspiration, `stomResist` affecting timing and magnitude of evapotranspiration, `snowIncept` affecting the magnitude canopy interception of snow, and `windPrfile` affecting the timing and magnitude of SWE, and latent and sensible heat. We will not explore the `groundwatr` configurations here as the differences show up only in most simulated variables post-calibration. This study does not examine models calibrated for every set up of forcings configurations. Note, if you want to look at `qTopmodl`, you must set  `bcLowrSoiH` to `zeroFlux` (we will leave it at `drainage`) and `hc_profile` to `pow_prof` (we will leave it at `constant`). You can add other configuration choices here; this notebook and the next notebook will work properly (but it will make the computations take longer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " if config_prob or lhs_config_prob==1:\n",
    "    file_manager = settings_folder+'/file_manager_truth.txt'\n",
    "    s = ps.Simulation(executable, file_manager)\n",
    "    s.manager['simStartTime'] = str(the_start)\n",
    "    s.manager['simEndTime'] = str(the_end)  \n",
    "    #Before running the ensemble that changes configuration we must write the original simulation's configuration.\n",
    "    s.manager.write()    \n",
    "    print(s.decisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_prob or lhs_config_prob==1:\n",
    "    #alld = {'groundwatr':np.array(['qTopmodl','bigBuckt']),\n",
    "    alld = {'stomResist':np.sort(np.array(['BallBerry','Jarvis'])),\n",
    "            'snowIncept':np.sort(np.array(['stickySnow', 'lightSnow'])),\n",
    "            'windPrfile':np.sort(np.array(['exponential','logBelowCanopy']))}\n",
    "    config = ps.ensemble.decision_product(alld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "The ensemble uses `++` as a delimiter to create unique identifiers for each simulation in the ensemble. The default configuration will be run again. We do this so that each finished SUMMA *.nc output file is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now, run these on with the truth forcing. This takes about 0.8 minoots to run with 1 HRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# truth, parameter space with 8 configurations\n",
    "if config_prob==1 and debug==0:\n",
    "    param_ens = ps.Ensemble(executable, config, file_manager, num_workers=NCORES)   \n",
    "    param_ens.run('local')\n",
    "    all_status = [(n, s.status) for n, s in param_ens.simulations.items()] #if want to look at status if has errors\n",
    "    all_ds = [s.output.load() for n, s in param_ens.simulations.items()] #load it into memory so faster  \n",
    "    all_name = [n for n, s in param_ens.simulations.items()]\n",
    "    all_merged = xr.concat(all_ds, pd.Index(all_name, name=\"decision\"))\n",
    "    all_merged.to_netcdf(top_folder+'/output/merged_day/NLDAStruth_configs.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all were a success\n",
    "if config_prob==1 and debug==0:\n",
    "    print(all_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "As with the parameter space problem, we plot cumulative variables to see how differences are compounding.\n",
    "Again, we plot one HRU (the first one) for 2 months and we are showing winter months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cummulative\n",
    "if config_prob==1 and debug==0:\n",
    "    fig, axes = plt.subplots(nrows=14, ncols=1, figsize=(20, 40))\n",
    "    axes = axes.flatten()\n",
    "    axes[0].set_title('Cumulative')\n",
    "\n",
    "    variables = list(all_merged.variables.keys())[7:22]\n",
    "\n",
    "    #start =  24*3*30 #summer\n",
    "    start =  24*9*30 #winter\n",
    "    stop = start + 2*30*24 \n",
    "    truth_plt = all_merged.isel(hru=0, time=slice(start, stop))\n",
    "\n",
    "    for idx, var in enumerate(variables[0:14]):\n",
    "        for i, dec in enumerate(all_name):    \n",
    "            truth_plt[var].isel(decision=i).plot(ax=axes[idx],label=dec)\n",
    "        axes[idx].set_title('') \n",
    "        axes[idx].set_ylabel(var)\n",
    "        axes[idx].set_xlabel('Date')\n",
    "    plt.tight_layout()\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete things to save memory as before\n",
    "if config_prob==1 and debug==0:\n",
    "    del param_ens\n",
    "    del all_ds \n",
    "    del all_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now run the other forcing sets as a loop. This takes about 7 X 0.8 minoots to run with 1 HRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Each forcing constant, with 8 configurations.\n",
    "if config_prob==1 and debug==0:\n",
    "    constant_vars= ['airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd']\n",
    "    for v in constant_vars:\n",
    "        ! rm -rf {ps_working}\n",
    "        file_manager = settings_folder+'/file_manager_constant_' + v +'.txt'\n",
    "        s = ps.Simulation(executable, file_manager)\n",
    "        s.manager['simStartTime'] = str(the_start)\n",
    "        s.manager['simEndTime'] = str(the_end) \n",
    "        s.manager.write()           \n",
    "        param_ens = ps.Ensemble(executable, config, file_manager, num_workers=NCORES)   \n",
    "        param_ens.run('local')\n",
    "        all_status = [(n, s.status) for n, s in param_ens.simulations.items()] #if want to look at status if has errors\n",
    "        all_ds = [s.output.load() for n, s in param_ens.simulations.items()] #load it into memory so faster  \n",
    "        all_name = [n for n, s in param_ens.simulations.items()]\n",
    "        all_merged = xr.concat(all_ds, pd.Index(all_name, name=\"decision\"))\n",
    "        all_merged.to_netcdf(top_folder+'/output/merged_day/NLDASconstant_' + v +'_configs.nc')\n",
    "        del param_ens\n",
    "        del all_ds \n",
    "        del all_merged\n",
    "        gc.collect()\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "If you have errors, debug mode will run a short time-period in non-ensemble (i.e. standard) pysumma mode. Set  `debug = 1` in the complexity choices. This will provide more detailed error messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_prob==1 and debug==1:\n",
    "    for key0 in set(config):\n",
    "        s = ps.Simulation(executable, file_manager)\n",
    "        s.manager['simStartTime'] = str(the_start)\n",
    "        s.manager['simEndTime'] = str(the_end_debug)\n",
    "        for key1 in set(config[key0]):\n",
    "            for key in set(config[key0].get(key1)):\n",
    "                s.decisions[key].values = config[key0].get(key1).get(key)\n",
    "        s.run('local', run_suffix='_default')\n",
    "        assert s.status == 'Success'\n",
    "        print(key0, s.status) #key0 is the configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Run the Full Problem\n",
    "\n",
    "This code will give the full problem, exploring the parameter space and the configurations together.\n",
    "The next notebook in the series runs the most complete figures using this full problem, but parts of the figures can be drawn with the simpler problems. \n",
    "First, we combine the decision sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make ensembles with parameter space (numl parameter sets plus 1 for default), should make 88\n",
    "if lhs_config_prob==1:\n",
    "    config_latin = {}\n",
    "    for key_config in config.keys():\n",
    "        c = config[key_config]\n",
    "        for key_latin in latin.keys():\n",
    "            l = latin[key_latin]\n",
    "            config_latin[key_config+key_latin] = {**c,**l}\n",
    "    print(len(config_latin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now we run the truth problem. This takes about 7.4 minoots to run with 1 HRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# truth, parameter space with 8 configurations\n",
    "if lhs_config_prob==1 and debug==0:\n",
    "    param_ens = ps.Ensemble(executable, config_latin, file_manager, num_workers=NCORES)   \n",
    "    param_ens.run('local')\n",
    "    all_status = [(n, s.status) for n, s in param_ens.simulations.items()] #if want to look at status if has errors\n",
    "    all_ds = [s.output.load() for n, s in param_ens.simulations.items()] #load it into memory so faster  \n",
    "    all_name = [n for n, s in param_ens.simulations.items()]\n",
    "    all_merged = xr.concat(all_ds, pd.Index(all_name, name=\"decision\"))\n",
    "    all_merged.to_netcdf(top_folder+'/output/merged_day/NLDAStruth_configs_latin.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all were a success\n",
    "if lhs_config_prob==1 and debug==0:\n",
    "    print(all_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Again, we plot the cumulatative output for one HRU (the first one) for 2 winter months, to see how differences are compounding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot cummulative\n",
    "if lhs_config_prob==1 and debug==0:\n",
    "    fig, axes = plt.subplots(nrows=14, ncols=1, figsize=(20, 40))\n",
    "    axes = axes.flatten()\n",
    "    axes[0].set_title('Cumulative')\n",
    "\n",
    "    variables = list(all_merged.variables.keys())[7:22]\n",
    "\n",
    "    #start =  24*3*30 #summer\n",
    "    start =  24*9*30 #winter\n",
    "    stop = start + 2*30*24 \n",
    "    truth_plt = all_merged.isel(hru=0, time=slice(start, stop))\n",
    "\n",
    "    for idx, var in enumerate(variables[0:14]):\n",
    "        for i, dec in enumerate(all_name):    \n",
    "            truth_plt[var].isel(decision=i).plot(ax=axes[idx],label=dec)\n",
    "        axes[idx].set_title('') \n",
    "        axes[idx].set_ylabel(var)\n",
    "        axes[idx].set_xlabel('Date')\n",
    "    plt.tight_layout()\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete things to save memory as before\n",
    "if lhs_config_prob==1 and debug==0:\n",
    "    del param_ens\n",
    "    del all_ds \n",
    "    del all_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Here, we run all the forcings. This takes about 7 X 7.4 minoots to run with 1 HRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Each forcing constant, parameter space with 8 configurations.\n",
    "if lhs_config_prob==1 and debug==0:\n",
    "    constant_vars= ['airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd']\n",
    "    for v in constant_vars:\n",
    "        ! rm -rf {ps_working}\n",
    "        file_manager = settings_folder+'/file_manager_constant_' + v +'.txt'\n",
    "        s = ps.Simulation(executable, file_manager)\n",
    "        s.manager['simStartTime'] = str(the_start)\n",
    "        s.manager['simEndTime'] = str(the_end)  \n",
    "        s.manager.write()       \n",
    "        param_ens = ps.Ensemble(executable, config_latin, file_manager, num_workers=NCORES)   \n",
    "        param_ens.run('local')\n",
    "        all_status = [(n, s.status) for n, s in param_ens.simulations.items()] #if want to look at status if has errors\n",
    "        all_ds = [s.output.load() for n, s in param_ens.simulations.items()] #load it into memory so faster  \n",
    "        all_name = [n for n, s in param_ens.simulations.items()]\n",
    "        all_merged = xr.concat(all_ds, pd.Index(all_name, name=\"decision\"))\n",
    "        all_merged.to_netcdf(top_folder+'/output/merged_day/NLDASconstant_' + v +'_configs_latin.nc')\n",
    "        del param_ens\n",
    "        del all_ds \n",
    "        del all_merged\n",
    "        gc.collect()\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "If you have errors, debug mode will run a short time-period in non-ensemble (i.e. standard) pysumma mode. Set `debug = 1` in the complexity choices. This will provide more detailed error messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_config_prob==1 and debug==1:\n",
    "    for key0 in set(config):\n",
    "        s = ps.Simulation(executable, file_manager)\n",
    "        s.manager['simStartTime'] = str(the_start)\n",
    "        s.manager['simEndTime'] = str(the_end_debug)\n",
    "        for key1 in set(config[key0]):\n",
    "            for key in set(config[key0].get(key1)):\n",
    "                s.decisions[key].values = config[key0].get(key1).get(key)\n",
    "        s.run('local', run_suffix='_default')\n",
    "        assert s.status == 'Success'\n",
    "        print(0, key0, s.status) #0 is default parameter set, key0 is the configuration\n",
    "\n",
    "    for j in range(0,numl):\n",
    "        for key0 in set(config):\n",
    "            s = ps.Simulation(executable, file_manager)\n",
    "            s.manager['simStartTime'] = str(the_start)\n",
    "            s.manager['simEndTime'] = str(the_end_debug)\n",
    "            for key in set(samples):\n",
    "                s.trial_params[key].values =  samples.get(key)[np.arange(j, len(the_hru)*numl, numl)]\n",
    "            for key1 in set(config[key0]):\n",
    "                for key in set(config[key0].get(key1)):\n",
    "                    s.decisions[key].values = config[key0].get(key1).get(key)\n",
    "            s.run('local', run_suffix='_default')\n",
    "            assert s.status == 'Success'\n",
    "            print(j+1, key0, s.status) #j+1 is the number of the LHS parameter set, key0 is the configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Compute Error on Output\n",
    "We calculate KGE statistics on the data. KGE means perfect agreement if it is 1, and <0 means the mean is a better guess. We use a modified KGE that avoids the amplified simulated mean divided by truth mean values when is the truth mean is small, and avoids the dependence of the KGE metric on the units of measurement. Then, we scale the KGE so that the range is 1 to -1.\n",
    "If the values are identical we use KGE of 1.\n",
    "We also keep summaries of the raw data (summed over time). \n",
    "This can take some time depending on how big of a problem you ran. It takes about 1/100th of the time it took to run the whole problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set forcings and create dictionaries, reordered forcings and output variables to match paper \n",
    "constant_vars= ['pptrate','airtemp','spechum','SWRadAtm','LWRadAtm','windspd','airpres'] \n",
    "allforcings = constant_vars+['truth']\n",
    "comp_sim=['scalarInfiltration','scalarlTotaRunoff','scalarAquiferBaseflow','scalarSoilDrainage',\n",
    "          'scalarTotalSoilWat','scalarCanopyWat','scalarLatHeatTotal','scalarTotalET','scalarSurfaceRunoff',\n",
    "          'scalarSWE','scalarRainPlusMelt','scalarSnowSublimation','scalarSenHeatTotal','scalarNetRadiation']\n",
    "var_sim = np.concatenate([constant_vars, comp_sim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitions for KGE computation, correlation with a constant (e.g. all SWE is 0) will be 0 here, not NA\n",
    "def covariance(x,y,dims=None):\n",
    "    return xr.dot(x-x.mean(dims), y-y.mean(dims), dims=dims) / x.count(dims)\n",
    "\n",
    "def correlation(x,y,dims=None):#\n",
    "    return (covariance(x,y,dims)) / (x.std(dims) * y.std(dims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "We will make an error summary for every problem you ran, so this can make up to 4 sets of error summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Names for each set of problem complexities.\n",
    "choices = [lhs_config_prob,lhs_prob,config_prob,default_prob]\n",
    "suffix = ['_configs_latin.nc','_latin.nc','_configs.nc','_hru.nc']\n",
    "\n",
    "for i,k in enumerate(choices):\n",
    "    if k==0: continue\n",
    "    sim_truth = xr.open_dataset(top_folder+'/output/merged_day/NLDAStruth'+suffix[i])\n",
    "    \n",
    "# Get decision names off the files\n",
    "    if i<3: decision_set = np.array(sim_truth['decision']) \n",
    "    if i==3: decision_set = np.array(['default'])\n",
    "\n",
    "# set up error calculations\n",
    "    summary = ['KGE','raw']\n",
    "    shape = ( len(decision_set),len(the_hru), len(allforcings),len(summary))\n",
    "    dims = ('decision','hru','var','summary')\n",
    "    coords = {'decision':decision_set,'hru': the_hru, 'var':allforcings, 'summary':summary}\n",
    "    error_data = xr.Dataset(coords=coords)\n",
    "    for s in comp_sim:\n",
    "        error_data[s] = xr.DataArray(data=np.full(shape, np.nan),\n",
    "                                     coords=coords, dims=dims,\n",
    "                                     name=s)\n",
    "        \n",
    "# calculate summaries\n",
    "    truth0_0 = sim_truth.drop_vars('hruId').load()\n",
    "    for v in constant_vars:\n",
    "        truth = truth0_0\n",
    "        truth = truth.isel(time = slice(initialization_days*24,None)) #don't include first year, 5 years\n",
    "        sim = xr.open_dataset(top_folder+'/output/merged_day/NLDASconstant_' + v + suffix[i])\n",
    "        sim = sim.drop_vars('hruId').load()\n",
    "        sim = sim.isel(time = slice(initialization_days*24,None)) #don't include first year, 5 years\n",
    "        r = sim.mean(dim='time') #to set up xarray since xr.dot not supported on dataset and have to do loop\n",
    "        for s in var_sim:         \n",
    "            r[s] = correlation(sim[s],truth[s],dims='time')\n",
    "        ds = 1 - np.sqrt( np.square(r-1) \n",
    "        + np.square( sim.std(dim='time')/truth.std(dim='time') - 1) \n",
    "        + np.square( (sim.mean(dim='time') - truth.mean(dim='time'))/truth.std(dim='time') ) )\n",
    "        for s in var_sim:   \n",
    "            #if constant and identical, want this as 1.0 -- correlation with a constant = 0 and std dev = 0\n",
    "            for h in the_hru:\n",
    "                if i<3: \n",
    "                        for d in decision_set:  \n",
    "                            ss = sim[s].sel(hru=h,decision = d)\n",
    "                            tt = truth[s].sel(hru=h,decision = d)\n",
    "                            ds[s].loc[d,h] =ds[s].sel(hru=h,decision = d).where(np.allclose(ss,tt, atol = 1e-10)==False, other=1.0)\n",
    "                else:\n",
    "                    ss = sim[s].sel(hru=h)\n",
    "                    tt = truth[s].sel(hru=h)\n",
    "                    ds[s].loc[h] =ds[s].sel(hru=h).where(np.allclose(ss,tt, atol = 1e-10)==False, other=1.0)\n",
    "\n",
    "        ds = ds/(2.0-ds)\n",
    "        ds0 = ds.load()\n",
    "        for s in comp_sim:\n",
    "            error_data[s].loc[:,:,v,'KGE']  = ds0[s]\n",
    "            error_data[s].loc[:,:,v,'raw']  = sim[s].sum(dim='time') #this is raw data, not error\n",
    "        print(v)\n",
    "    for s in comp_sim:\n",
    "        error_data[s].loc[:,:,'truth','raw']  = truth[s].sum(dim='time') #this is raw data, not error      \n",
    "        \n",
    "#save file\n",
    "    error_data.to_netcdf(regress_folder+'/error_data'+suffix[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysumma",
   "language": "python",
   "name": "pysumma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
