{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the SUMMA setups\n",
    "Here, we run SUMMA using pysumma, on the setupss we created in the previous notebook.\n",
    "This is a computationally expensive notebook. \n",
    "If you don't have the computer power or time, you can run a less complex problem than the full problem, but the output (and graphs) in the next notebook will not be complete. \n",
    "If things start to fail, restart the server and run the simuluations in smaller loops before restarts. \n",
    "The last section of the notebook computes summary statistics of the output to be used in the next notebook.\n",
    "\n",
    "Complexity choices are, in order of increasing complexity: \n",
    " - 1)   `default_prob = 1`: the \"default\" configuration with the \"default\" parameters. By \"default\" we mean whatever you chose in the summa setup files. \n",
    " - 2a) `lhs_prob = 1`: the default configuration with exploration of the parameter space.\n",
    " - 2b) `config_prob = 1`: the default parameters with 8 different configurations (choices that have been seen to affect the model output in previous research) \n",
    " - 3)   `lhs_config_prob = 1`: 8 different configurations with the default and the exploration of the parameter space.\n",
    "\n",
    "The default problem will be run with every cmplexity choice.\n",
    "\n",
    "Eight iterations of each loop are run for each problem, to cover a truth run and the 7 forcings each held to a daily constant in turn.\n",
    "The time unit in the commnets given is written as \"minoot\", where 1 minoot= 1 minute on the CyberGIS for Water Xcede, but will be different on your computer or another computer. \n",
    "For comparision, one iteration of each problem (in order of increasing complexity) takes: \n",
    " - 1)   8X ~0.3 minoots for 1 HRUs\n",
    " - 2a)  8X ~1.2 minoots for 1 HRU\n",
    " - 2b)  8X ~0.8 minoots for 1 HRU\n",
    " - 3)   8X ~73 minoots for 1 HRU\n",
    " \n",
    "There are times in \"minoots\" for various cells in the notebook, so you can have an idea how long a cell might take to run. \n",
    " \n",
    "You can also make the problem run for fewer years to lower computational costs, changing `str(the_start)` and `str(the_end)`. You can test with as little as a day between `the_start` and `the_end`, but if you want to make the plots in this notebook and run the next notebook you will need >1 year (1 year is considered intialization period). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Make problem complexity choices here:\n",
    " \n",
    "Note, you can choose all of these to be 1, but each step of complexity will contain the previous problem(s), so it is not necessary. It will result in more files, but if may be useful to check that each step of the problem expansion runs successfullly. If you have more than 10 HRUs (CAMELS subbasins in the example problem), it was decided that the problem is too big and you can only run the default problem (the complexity choice will get reset after the number of HRUs is determined). However, theoretically it will run with more HRUs. \n",
    "\n",
    "Another complexity choice you can make is to run a different length time-period. It is pre-populated to run 18 months, as the 6 years in the paper takes a long time without high-performance computing. You also need to choose how long the initialization period is for the error calculations. We suggest 183 to 365 days, with at least 1 more year of simulation. So for example, if you run 18 months of simulation you should choose your initialization to be 183 days.\n",
    "\n",
    "Lastly, if you have any errors while running the pysumma problems, you can run a short time-period (1 day) in simpler pysumma mode by changing `debug` to 1 and rerunning. This will provide more detailed error messages. The problem WILL NOT run for the full time period if `debug = 1`. The initialization period for error calculation will be set to 0 days (but the plots based on these error calculations in the next notebook would be uninformative). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prob = 0\n",
    "lhs_prob = 0\n",
    "config_prob = 0\n",
    "lhs_config_prob = 1 #pre-populated this is 1 and the rest 0, to run full problem that is in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_start = '1990-10-01 00:00' #pre-populated to '1990-10-01 00:00' as in the paper.\n",
    "the_end =   '1996-09-30 23:00' #pre-populated to '1996-09-30 23:00' is in the paper.\n",
    "initialization_days = 365 #pre-populated 365 days as in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 1, ONLY the one day problem with more detailed error messages will not run. \n",
    "# if 0, the problem will run from the_start to the_end as defined above.\n",
    "debug = 0 #pre-populated to 0\n",
    "the_end_debug = the_start[0:11] + '23:00'\n",
    "if debug==1: initialization_days = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Check that we loaded correct environment. This should show pysumma version 3.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda list summa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Load the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pysumma as ps\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "from natsort import natsorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Set up the paths and regionalize the paths in the configuration files that SUMMA will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_folder = os.path.join(os.getcwd(), 'summa_camels')\n",
    "settings_folder = os.path.join(top_folder, 'settings')\n",
    "ps_working = os.path.join(top_folder, '.pysumma')\n",
    "regress_folder = os.path.join(os.getcwd(), 'regress_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_line_startwith(lines, flag, new_line_replacement):\n",
    "    for i in range(len(lines)):\n",
    "        if lines[i].startswith(flag):\n",
    "            lines[i] = new_line_replacement\n",
    "    return lines\n",
    "temple_filemanager = os.path.join(settings_folder, \"template_file_manager.1hr.txt\")\n",
    "with open(temple_filemanager, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    new_lines1 = replace_line_startwith(lines, \"simStartTime\", \"simStartTime         '{the_start}' !\\n\".format(the_start=the_start))\n",
    "    new_lines2 = replace_line_startwith(new_lines1, \"simEndTime\", \"simEndTime           '{the_end}' !\\n\".format(the_end=the_end))\n",
    "f = open(os.path.join(settings_folder, \"template_file_manager.1hr.txt\"), \"w\")\n",
    "#f.writelines(new_lines1)\n",
    "f.writelines(new_lines2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd {top_folder}; chmod +x installTestCases_local.sh; ./installTestCases_local.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of HRUs\n",
    "attrib = xr.open_dataset(settings_folder+'/attributes.nc')\n",
    "the_hru = np.array(attrib['hruId'])\n",
    "# change complexity if necessary\n",
    "if len(the_hru) >10:\n",
    "    default_prob = 1    \n",
    "    lhs_prob = 0\n",
    "    config_prob = 0\n",
    "    lhs_config_prob = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Interacting with SUMMA via the `Distributed` object\n",
    "\n",
    "If we have more than one basin, we are running a `Distributed` object, which has multiple `Simulation` objects inside, each corresponding to some spatial chunk. (If we only have one basin, we cannot run this way and then must run as a single `Simulation` object.) \n",
    "We need to do `rm -r {ps_working}` to clear out the distributed folders every run so permissions do not get screwed up in the loops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fewer basins, do not exceed number of basins in chunking for Default problem\n",
    "CHUNK = 8 #for all 671 basins or more than 8\n",
    "CHUNK0 = CHUNK\n",
    "if len(the_hru) <8: CHUNK0 = len(the_hru)\n",
    "NCORES=6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "To set up a `Distributed` object you must supply several pieces of information. \n",
    "First, supply the SUMMA executable; this could be either the compiled executable on your local machine, or a docker image. \n",
    "The second piece of information is the path to the file manager, which we just created through the install script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable =  '/usr/bin/summa.exe'  #     \n",
    "file_manager = settings_folder+'/file_manager_truth.txt'\n",
    "if CHUNK0>1: camels = ps.Distributed(executable, file_manager, num_workers=NCORES, chunk_size=CHUNK0)\n",
    "if CHUNK0==1: camels = ps.Simulation(executable, file_manager)\n",
    "camels.manager['simStartTime'] = str(the_start)\n",
    "camels.manager['simEndTime'] = str(the_end)\n",
    "camels.manager.write()\n",
    "print(camels.manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Run Problem with Truth Forcing\n",
    "\n",
    "We run pysumma with NLDAS on each basin, the 'truth run'. You can check how long it has been running by using the command `qstat -u <username>` in a terminal. This takes about 0.3 minoots to run with 1 HRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# truth, default parameters and configuration\n",
    "if default_prob==1 and debug==0:\n",
    "    camels.run('local')\n",
    "    if CHUNK0>1:\n",
    "        all_status = [(n, s.status) for n, s in camels.simulations.items()] #if want to look at status if has errors\n",
    "        all_ds = [s.output.load() for n, s in camels.simulations.items()] #load it into memory so faster\n",
    "    if CHUNK0==1:\n",
    "        all_status = camels.status #if want to look at status if has errors\n",
    "        all_ds = camels.output.load() #load it into memory so faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "We will merge the GRU and HRU files if we ran the distibuted. We could just write it as several files instead of merging. However, if we want to merge, we can do the following.\n",
    "First, detect automatically which vars have hru vs gru dimensions (depending on what we use for output, we may not have any gru):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if default_prob==1 and debug==0:\n",
    "    hru_vars = [] # variables that have hru dimension\n",
    "    gru_vars = [] # variables that have gru dimension\n",
    "    if CHUNK0>1:\n",
    "        for ds in all_ds:\n",
    "            for name, var in ds.variables.items():\n",
    "                if 'hru' in var.dims:\n",
    "                    hru_vars.append(name)\n",
    "                elif 'gru' in var.dims:\n",
    "                    gru_vars.append(name)\n",
    "    if CHUNK0==1:\n",
    "        for name, var in all_ds.variables.items():\n",
    "            if 'hru' in var.dims:\n",
    "                hru_vars.append(name)\n",
    "            elif 'gru' in var.dims:\n",
    "                gru_vars.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "If we ran distributed, filter variables for merge, write merged files, and delete stuff to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if default_prob==1 and debug==0: \n",
    "    if CHUNK0>1:\n",
    "        hru_ds = [ds[hru_vars] for ds in all_ds]\n",
    "        gru_ds = [ds[gru_vars] for ds in all_ds]\n",
    "        hru_merged = xr.concat(hru_ds, dim='hru')\n",
    "        gru_merged = xr.concat(gru_ds, dim='gru')\n",
    "    if CHUNK0==1:\n",
    "        hru_merged = all_ds[hru_vars]\n",
    "        gru_merged = all_ds[gru_vars]\n",
    "    print(hru_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if default_prob==1 and debug==0:\n",
    "    hru_merged.to_netcdf(top_folder+'/output/merged_day/NLDAStruth_hru.nc')\n",
    "    gru_merged.to_netcdf(top_folder+'/output/merged_day/NLDAStruth_gru.nc')\n",
    "    del camels\n",
    "    del all_ds \n",
    "    del hru_merged\n",
    "    del gru_merged\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Run Problem with Constant Forcing\n",
    "\n",
    "Here are the other runs with each forcing held constant, now as a loop. \n",
    "We delete stuff after every run to reduce memory needs. This takes about 7 X 0.3 minoots to run with 1 HRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\"\n",
    "    Credit:\n",
    "    https://stackoverflow.com/questions/26646362/numpy-array-is-not-json-serializable\n",
    "    \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import shutil, os\n",
    "!rm -rf summa_camels.zip\n",
    "workspace_dir = os.path.join(os.getcwd(), 'workspace')\n",
    "!mkdir -p {workspace_dir}\n",
    "unzip_dir = tempfile.mkdtemp(dir=workspace_dir)\n",
    "model_folder_name = \"summa_camels\"\n",
    "model_folder = os.path.join(unzip_dir, model_folder_name)\n",
    "shutil.make_archive(model_folder_name, 'zip', os.getcwd()+\"/summa_camels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Start HPC use  </b> (default_prob) default_prob SUMMA runs with each forcing (7 simulations) </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zip SUMMA Setup and move it to workspace folder for HPC use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if default_prob==1 and debug==0:\n",
    "    workspace_dir = os.path.join(os.getcwd(), 'workspace')\n",
    "    !mkdir -p {workspace_dir}\n",
    "    unzip_dir = tempfile.mkdtemp(dir=workspace_dir)\n",
    "    model_folder_name = \"summa_camels\"\n",
    "    model_folder = os.path.join(unzip_dir, model_folder_name)\n",
    "    !unzip -o {model_folder_name}.zip -d {model_folder}\n",
    "    !rm -rf {model_folder}/output\n",
    "    !mkdir {model_folder}/output\n",
    "    !mkdir {model_folder}/output/constant\n",
    "    !mkdir {model_folder}/output/merged_day\n",
    "    !mkdir {model_folder}/output/truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_vars= ['airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd']\n",
    "f_config = {}\n",
    "for i, v in enumerate(constant_vars):\n",
    "    key = 'run'+str(i)\n",
    "    value = {'file_manager': '<PWD>/settings/file_manager_constant_' + v +'.txt'}\n",
    "    f_config[key] = value\n",
    "f_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if default_prob==1 and debug==0:\n",
    "    import json\n",
    "    with open(os.path.join(model_folder, 'summa_options.json'), 'w') as outfile:\n",
    "        json.dump(f_config, outfile)\n",
    "\n",
    "    # check ensemble parameters    \n",
    "    print(\"Number of ensemble runs: {}\".format(len(f_config)))\n",
    "    print(json.dumps(f_config, indent=4, sort_keys=True)[:800])\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit model to HPC using New Job Submission Service Python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if default_prob==1 and debug==0:\n",
    "    from job_supervisor_client import *\n",
    "    communitySummaSession = Session('summa', isJupyter=True)\n",
    "    communitySummaJob = communitySummaSession.job() # create new job\n",
    "    communitySummaJob.upload(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if default_prob==1 and debug==0:\n",
    "    communitySummaJob.submit(payload={\n",
    "        \"node\": 7,\n",
    "        \"machine\": \"keeling\",\n",
    "        \"file_manager_rel_path\": \"settings/file_manager_constant_airpres.txt\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if default_prob==1 and debug==0:\n",
    "    communitySummaJob.events(liveOutput=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if default_prob==1 and debug==0:\n",
    "    job_dir = os.path.join(model_folder, \"{}\".format(communitySummaJob.id))\n",
    "    !mkdir -p {job_dir}/output\n",
    "    communitySummaJob.download(job_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if default_prob==1 and debug==0:\n",
    "    !cd {job_dir} && unzip *.zip -d output\n",
    "\n",
    "    # check output directory\n",
    "    output_path = os.path.join(job_dir, \"output/constant\")\n",
    "    # check SUMMA output file \n",
    "    name_list = os.listdir(output_path)\n",
    "    full_list = [os.path.join(output_path,i) for i in name_list if i.endswith(\".nc\")]\n",
    "    sorted_list = sorted(full_list)\n",
    "    sorted_list = sorted(sorted_list, key=lambda v: v.upper())\n",
    "\n",
    "    for f in sorted_list:\n",
    "        print(f)\n",
    "    print(\"Number of NC files: {}\".format(len(sorted_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Each forcing constant, default parameters and configuration\n",
    "if default_prob==1 and debug==0:\n",
    "    constant_vars= ['airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd']\n",
    "    for i, v in enumerate(constant_vars):\n",
    "        #all_ds = xr.open_dataset(sorted_list[i]).load()   \n",
    "        hru_vars = [] # variables that have hru dimension\n",
    "        gru_vars = [] # variables that have gru dimension\n",
    "        #for ds in xr.open_dataset(sorted_list[i]):\n",
    "        for name, var in xr.open_dataset(sorted_list[i]).variables.items():\n",
    "            if 'hru' in var.dims:\n",
    "                hru_vars.append(name)\n",
    "            elif 'gru' in var.dims:\n",
    "                gru_vars.append(name)\n",
    "        hru_ds = xr.open_dataset(sorted_list[i])[hru_vars]\n",
    "        gru_ds = xr.open_dataset(sorted_list[i])[gru_vars]\n",
    "        hru_ds.to_netcdf(top_folder+'/output/merged_day/NLDASconstant_' + v +'_hru.nc')\n",
    "        gru_ds.to_netcdf(top_folder+'/output/merged_day/NLDASconstant_' + v +'_gru.nc')\n",
    "        gc.collect()\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Finish HPC use  </b> (default_prob) default_prob SUMMA runs with each forcing (7 simulations) </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Exploring the Parameter Calibration Space with a Latin Hypercube\n",
    "\n",
    "The above models were run on the default parameter set only. Let's rerun with parameter sets selected by using a Latin Hypercube to get 10 different parameter sets for every HRU, in order to explore the calibration space. This will show us if the results of forcing importance could change after calibration. \n",
    "\n",
    "Currrently, none of the model parameters (or decisions) can be altered in a `Distributed` object. \n",
    "However, if we switch to `Simulation` objects and use the `Ensemble` class, we can run suites of different model parameters with relative ease. \n",
    "\n",
    "We change only the parameters that are usually calibrated. You can remove parameters if you were not planning to ever calibrate them away from their defaults (likewise you could add parameters).\n",
    "\n",
    "The absolute minimums and maximums will break simulations and zero out variables, so we do not use those, we stay at the 5% level away from the extremes. Also, there are some constraints on the parameters that must be followed, they are:\n",
    "\n",
    "* heightCanopyTop   > heightCanopyBottom\n",
    "* critSoilTranspire > theta_res\n",
    "* theta_sat         > critSoilTranspire\n",
    "* fieldCapacity     > theta_res\n",
    "* theta_sat         > fieldCapacity\n",
    "* theta_sat         > theta_res\n",
    "* critSoilTranspire > critSoilWilting\n",
    "* critSoilWilting   > theta_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_prob==1 or lhs_config_prob==1: from pyDOE import lhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    file_manager = settings_folder+'/file_manager_truth.txt'\n",
    "    s = ps.Simulation(executable, file_manager)\n",
    "    s.manager['simStartTime'] = str(the_start)\n",
    "    s.manager['simEndTime'] = str(the_end)  \n",
    "    #Before running the ensemble that changes parameters we must write the original simulation's parameters.\n",
    "    s.manager.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the default, min, and max as in /settings.v1/localParamInfo.txt and /settings.v1/basinParamInfo.txt\n",
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    param_calib_hru = ['albedoRefresh', 'aquiferBaseflowExp', 'aquiferBaseflowRate', 'frozenPrecipMultip', 'heightCanopyBottom','heightCanopyTop', 'k_macropore', \n",
    "                   'k_soil', 'qSurfScale', 'summerLAI', 'tempCritRain', 'theta_sat', 'windReductionParam'] \n",
    "    param_calib_gru = ['routingGammaScale', 'routingGammaShape']\n",
    "\n",
    "    for k in param_calib_hru:\n",
    "        print(s.global_hru_params[k])\n",
    "    for k in param_calib_gru:\n",
    "        print(s.global_gru_params[k]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    bounds_hru = np.full((len(param_calib_hru),3),1.0)\n",
    "    bounds_gru = np.full((len(param_calib_gru),3),1.0)\n",
    "    for i,k in enumerate(param_calib_hru): bounds_hru[i,]= s.global_hru_params.get_value(k)[0:3]\n",
    "    for i,k in enumerate(param_calib_gru): bounds_gru[i,]= s.global_gru_params.get_value(k)[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bounds and expand to size of LHS runs\n",
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    numl = 10\n",
    "    num_vars =  len(param_calib_hru) + len(param_calib_gru)\n",
    "    names =  param_calib_hru + param_calib_gru\n",
    "    bounds =  np.concatenate((bounds_hru, bounds_gru), axis=0)\n",
    "    par_def = dict(zip(names, np.transpose(np.tile(bounds[:,0],(len(the_hru),1))) ))\n",
    "    par_min = dict(zip(names, np.transpose(np.tile(bounds[:,1],(numl*len(the_hru),1))) ))\n",
    "    par_max = dict(zip(names, np.transpose(np.tile(bounds[:,2],(numl*len(the_hru),1))) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "We remove geographically distributed parameters from the default set, and then make the LHS parameter set plus deault from the above bounds. Set 0 will be the default parameter set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove geographically distributed parameters from default set\n",
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    distributed_val = par_def.pop('heightCanopyBottom')\n",
    "    distributed_val = par_def.pop('heightCanopyTop')\n",
    "    distributed_val = par_def.pop('k_soil')\n",
    "    distributed_val = par_def.pop('theta_sat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to obey parameter constraints\n",
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    param = xr.open_dataset(settings_folder+'/parameters.nc')\n",
    "\n",
    "    for i,h in enumerate(the_hru):\n",
    "        lb_theta_sat = max(param[['critSoilTranspire','fieldCapacity','theta_res']].isel(hru=i).values()).values\n",
    "        for j in range(0,numl): #say first numl belong to hru 0, second numl to hru 1, and so on\n",
    "            if (par_min['theta_sat'][j + i*numl]<lb_theta_sat): par_min['theta_sat'][j + i*numl]=lb_theta_sat\n",
    "\n",
    "    par_min['heightCanopyTop'] = par_max['heightCanopyBottom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a 5% buffer\n",
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    buff = {key: (par_max.get(key) - par_min.get(key))*0.05 for key in set(par_max) }\n",
    "    par_min = {key: par_min.get(key) + buff.get(key)*0.05 for key in set(buff) }\n",
    "    par_max = {key: par_max.get(key) - buff.get(key)*0.05 for key in set(buff) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate samples with Latin Hypercube Sampling, set seed by HRU ID so it is the same every time run\n",
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    lhd = np.empty(shape=(num_vars,numl*len(the_hru)))\n",
    "    for i, h in enumerate(the_hru):\n",
    "        np.random.seed(h) #if the hru ID is not a number this will not work\n",
    "        lhd[:,range(i*numl,(i+1)*numl)] = lhs(numl, samples=num_vars)\n",
    "    lhd = dict(zip(names,lhd))\n",
    "    samples = {key: par_min.get(key) + lhd.get(key)*(par_max.get(key) - par_min.get(key)) for key in set(par_max) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make ensembles\n",
    "if lhs_prob==1 or lhs_config_prob==1:\n",
    "    latin = {}\n",
    "    latin[str(0)] = {'trial_parameters': {key: par_def.get(key) for key in set(par_def) }}\n",
    "    for j in range(0,numl):\n",
    "            latin[str(j+1)] = {'trial_parameters': {key: samples.get(key)[np.arange(j, len(the_hru)*numl, numl)] for key in set(samples) }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Start HPC use  </b> (lhs_prob) lhs_prob SUMMA runs (11 simulations) </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if lhs_prob==1:\n",
    "    import tempfile\n",
    "    import shutil, os\n",
    "    workspace_dir = os.path.join(os.getcwd(), 'workspace')\n",
    "    !mkdir -p {workspace_dir}\n",
    "    unzip_dir = tempfile.mkdtemp(dir=workspace_dir)\n",
    "    model_folder_name = \"summa_camels\"\n",
    "    model_folder = os.path.join(unzip_dir, model_folder_name)\n",
    "    !unzip -o {model_folder_name}.zip -d {model_folder}\n",
    "    !rm -rf {model_folder}/output\n",
    "    !mkdir {model_folder}/output\n",
    "    !mkdir {model_folder}/output/constant\n",
    "    !mkdir {model_folder}/output/merged_day\n",
    "    !mkdir {model_folder}/output/truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_prob==1:\n",
    "    latin1 = json.dumps(latin, cls=NumpyEncoder)\n",
    "    latin = json.loads(latin1)\n",
    "    latin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if lhs_prob==1:\n",
    "    import json\n",
    "    with open(os.path.join(model_folder, 'summa_options.json'), 'w') as outfile:\n",
    "        json.dump(latin, outfile)\n",
    "\n",
    "    # check ensemble parameters    \n",
    "    print(\"Number of ensemble runs: {}\".format(len(latin)))\n",
    "    print(json.dumps(latin, indent=4, sort_keys=True)[:800])\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit model to HPC using New Job Submission Service Python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_prob==1:\n",
    "    from job_supervisor_client import *\n",
    "    communitySummaSession = Session('summa', isJupyter=True)\n",
    "    communitySummaJob = communitySummaSession.job() # create new job\n",
    "    communitySummaJob.upload(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_prob==1:\n",
    "    communitySummaJob.submit(payload={\n",
    "        \"node\": 11,\n",
    "        \"machine\": \"comet\",\n",
    "        \"file_manager_rel_path\": \"settings/file_manager_truth.txt\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if lhs_prob==1:\n",
    "    communitySummaJob.events(liveOutput=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When outputs are very big, sometimes missing data happened. So you need to reexecute this cell again.-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if lhs_prob==1:\n",
    "    job_dir = os.path.join(model_folder, \"{}\".format(communitySummaJob.id))\n",
    "    !mkdir -p {job_dir}/output\n",
    "    communitySummaJob.download(job_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if lhs_prob==1:\n",
    "    !cd {job_dir} && unzip *.zip -d output\n",
    "    # check output directory\n",
    "    output_path = os.path.join(job_dir, \"output/truth\")\n",
    "    # check SUMMA output file \n",
    "    name_list = os.listdir(output_path)\n",
    "    full_list1 = [os.path.join(output_path,i) for i in name_list if i.endswith(\".nc\")]\n",
    "    sorted_list = natsorted(full_list1)\n",
    "    sorted_list = natsorted(sorted_list, key=lambda v: v.upper())\n",
    "    for f in sorted_list:\n",
    "        print(f)\n",
    "    print(\"Number of NC files: {}\".format(len(sorted_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# truth, parameter space with default configuration\n",
    "if lhs_prob==1 and debug==0:\n",
    "    all_ds = [xr.open_dataset(f) for f in sorted_list]\n",
    "    all_name = np.array([str(elem) for elem in [n for n in range(len(latin))]])\n",
    "    all_name.astype('object')\n",
    "    all_merged = xr.concat(all_ds, pd.Index(all_name, name=\"decision\"))\n",
    "    all_merged.to_netcdf(top_folder+'/output/merged_day/NLDAStruth_latin.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_prob==1:\n",
    "    for i in range(len(sorted_list)):\n",
    "        shutil.copy(sorted_list[i], top_folder+'/output/truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cummulative\n",
    "if lhs_prob==1 and debug==0:\n",
    "    fig, axes = plt.subplots(nrows=14, ncols=1, figsize=(20, 40))\n",
    "    axes = axes.flatten()\n",
    "    axes[0].set_title('Cumulative')\n",
    "\n",
    "    variables = list(all_merged.variables.keys())[7:22]\n",
    "\n",
    "    #start =  24*3*30 #summer\n",
    "    start =  24*9*30 #winter\n",
    "    stop = start + 2*30*24 \n",
    "    truth_plt = all_merged.isel(hru=0, time=slice(start, stop))\n",
    "\n",
    "    for idx, var in enumerate(variables[0:14]):\n",
    "        for i, dec in enumerate(all_name):    \n",
    "            truth_plt[var].isel(decision=i).plot(ax=axes[idx],label=dec)\n",
    "        axes[idx].set_title('') \n",
    "        axes[idx].set_ylabel(var)\n",
    "        axes[idx].set_xlabel('Date')\n",
    "    plt.tight_layout()\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Finish HPC use  </b> (lhs_prob) lhs_prob SUMMA runs (11 simulations) </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Start HPC use  </b> (lhs_prob) lhs_prob SUMMA runs with each forcing (77 simulations) </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if lhs_prob==1:\n",
    "    import tempfile\n",
    "    import shutil, os\n",
    "    workspace_dir = os.path.join(os.getcwd(), 'workspace')\n",
    "    !mkdir -p {workspace_dir}\n",
    "    unzip_dir = tempfile.mkdtemp(dir=workspace_dir)\n",
    "    model_folder_name = \"summa_camels\"\n",
    "    model_folder = os.path.join(unzip_dir, model_folder_name)\n",
    "    !unzip -o {model_folder_name}.zip -d {model_folder}\n",
    "    !rm -rf {model_folder}/output\n",
    "    !mkdir {model_folder}/output\n",
    "    !mkdir {model_folder}/output/constant\n",
    "    !mkdir {model_folder}/output/merged_day\n",
    "    !mkdir {model_folder}/output/truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_prob==1:\n",
    "    config_lhc = {}\n",
    "    for k, v in f_config.items():\n",
    "        for k2, v2 in latin.items():\n",
    "            v_copy = v.copy()\n",
    "            v_copy.update(v2)\n",
    "            config_lhc[\"{}_{}\".format(k, k2)] = v_copy\n",
    "    config_lhc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if lhs_prob==1:\n",
    "    import json\n",
    "    with open(os.path.join(model_folder, 'summa_options.json'), 'w') as outfile:\n",
    "        json.dump(config_lhc, outfile)\n",
    "\n",
    "    # check ensemble parameters    \n",
    "    print(\"Number of ensemble runs: {}\".format(len(config_lhc)))\n",
    "    print(json.dumps(config_lhc, indent=4, sort_keys=True)[:800])\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit model to HPC using New Job Submission Service Python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_prob==1:\n",
    "    from job_supervisor_client import *\n",
    "    communitySummaSession = Session('summa', isJupyter=True)\n",
    "    communitySummaJob = communitySummaSession.job() # create new job\n",
    "    communitySummaJob.upload(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_prob==1:\n",
    "    communitySummaJob.submit(payload={\n",
    "        \"node\": 77,\n",
    "        \"machine\": \"comet\",\n",
    "        \"file_manager_rel_path\": \"settings/file_manager_constant_airpres.txt\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if lhs_prob==1:\n",
    "    communitySummaJob.events(liveOutput=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When outputs are very big, sometimes missing data happened. So you need to reexecute this cell again.-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if lhs_prob==1:\n",
    "    job_dir = os.path.join(model_folder, \"{}\".format(communitySummaJob.id))\n",
    "    !mkdir -p {job_dir}/output\n",
    "    communitySummaJob.download(job_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_prob==1:\n",
    "    !cd {job_dir} && unzip *.zip -d output\n",
    "    # check output directory\n",
    "    output_path = os.path.join(job_dir, \"output/constant\")\n",
    "    # check SUMMA output file \n",
    "    name_list = os.listdir(output_path)\n",
    "    full_list1 = [os.path.join(output_path,i) for i in name_list if i.endswith(\".nc\")]\n",
    "    sorted_list = natsorted(full_list1)\n",
    "    sorted_list = natsorted(sorted_list, key=lambda v: v.upper())\n",
    "    for f in sorted_list:\n",
    "        print(f)\n",
    "    print(\"Number of NC files: {}\".format(len(sorted_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# truth, parameter space with default configuration\n",
    "if lhs_prob==1:\n",
    "    i = 0\n",
    "    if lhs_prob==1:\n",
    "        constant_vars= ['airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd']\n",
    "        for v in constant_vars:    \n",
    "            all_ds = [xr.open_dataset(f) for f in sorted_list[i:i+int(len(sorted_list)/7)]]\n",
    "            all_name = np.array([str(elem) for elem in [n for n in range(int(len(name_list)/7))]])\n",
    "            all_name.astype('object')\n",
    "            all_merged = xr.concat(all_ds, pd.Index(all_name, name=\"decision\"))\n",
    "            all_merged.to_netcdf(top_folder+'/output/merged_day/NLDASconstant_' + v +'_latin.nc')\n",
    "            i = i + 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_prob==1:\n",
    "    for i in range(len(sorted_list)):\n",
    "        shutil.copy(sorted_list[i], top_folder+'/output/constant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Finish HPC use  </b> (lhs_prob) lhs_prob SUMMA runs with each forcing (77 simulations) </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Manipulating the Configuration of the pysumma Objects\n",
    "\n",
    "We need to run the parameter space with other model configurations, to see if the results seen on the default configuration hold true across the parameter space. The new configurations follow the exploration of [this paper.](https://doi.org/10.1002/2015WR017200).\n",
    "\n",
    "Clark, M.P., Nijssen, B., Lundquist, J.D., Kavetski, D., Rupp, D.E., Woods, R.A., Freer, J.E., Gutmann, E.D., Wood, A.W., Gochis, D.J. and Rasmussen, R.M., 2015. A unified approach for process‐based hydrologic modeling: 2. Model implementation and case studies. Water Resources Research, 51(4), pp.2515-2542.\n",
    "\n",
    "Of the model configurations discussed in this paper, the decisions that made the most difference are:\n",
    "\n",
    " - `groundwatr` choice of groundwater parameterization as:\n",
    "   - `qTopmodl` the topmodel parameterization (note must set hc_profile = pow_prof and bcLowrSoiH = zeroFlux\n",
    "   - `bigBuckt` a big bucket (lumped aquifer model) in between the other two choices for complexity\n",
    "   - `noXplict` no explicit groundwater parameterization\n",
    " - `stomResist` choice of function for stomatal resistance as:\n",
    "   - `BallBerry` Ball-Berry (1987) parameterization of physiological factors controlling transpiration\n",
    "   - `Jarvis` Jarvis (1976) parameterization of physiological factors controlling transpiration\n",
    "   - `simpleResistance` parameterized solely as a function of soil moisture limitations\n",
    " - `snowIncept` choice of parameterization for snow interception as:\n",
    "   - `stickySnow` maximum interception capacity is an increasing function of temperature\n",
    "   - `lightSnow` maximum interception capacity is an inverse function of new snow density\n",
    " - `windPrfile` choice of wind profile as:\n",
    "   - `exponential` an exponential wind profile extends to the surface\n",
    "   - `logBelowCanopy` a logarithmic profile below the vegetation canopy\n",
    "\n",
    "Choices `bigBuckt`, `BallBerry`, `lightSnow`, and `logBelowCanopy` are the defaults that we have run already (see the decisions printed out in the previous cell). The paper showed choice of `groundwatr` affecting the timing of runoff and the magnitude of evapotranspiration, `stomResist` affecting timing and magnitude of evapotranspiration, `snowIncept` affecting the magnitude canopy interception of snow, and `windPrfile` affecting the timing and magnitude of SWE, and latent and sensible heat. We will not explore the `groundwatr` configurations here as the differences show up only in most simulated variables post-calibration. This study does not examine models calibrated for every set up of forcings configurations. Note, if you want to look at `qTopmodl`, you must set  `bcLowrSoiH` to `zeroFlux` (we will leave it at `drainage`) and `hc_profile` to `pow_prof` (we will leave it at `constant`). You can add other configuration choices here; this notebook and the next notebook will work properly (but it will make the computations take longer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_prob or lhs_config_prob==1:\n",
    "    file_manager = settings_folder+'/file_manager_truth.txt'\n",
    "    s = ps.Simulation(executable, file_manager)\n",
    "    s.manager['simStartTime'] = str(the_start)\n",
    "    s.manager['simEndTime'] = str(the_end)  \n",
    "    #Before running the ensemble that changes configuration we must write the original simulation's configuration.\n",
    "    s.manager.write()    \n",
    "    print(s.decisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_prob or lhs_config_prob==1:\n",
    "    #alld = {'groundwatr':np.array(['qTopmodl','bigBuckt']),\n",
    "    alld = {'stomResist':np.sort(np.array(['BallBerry','Jarvis'])),\n",
    "            'snowIncept':np.sort(np.array(['stickySnow', 'lightSnow'])),\n",
    "            'windPrfile':np.sort(np.array(['exponential','logBelowCanopy']))}\n",
    "    config = ps.ensemble.decision_product(alld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "The ensemble uses `++` as a delimiter to create unique identifiers for each simulation in the ensemble. The default configuration will be run again. We do this so that each finished SUMMA *.nc output file is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now, run these on with the truth forcing. This takes about 0.8 minoots to run with 1 HRU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Start HPC use  </b> (config_prob) config_prob SUMMA runs (8 simulations) </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if config_prob:\n",
    "    import tempfile\n",
    "    import shutil, os\n",
    "    workspace_dir = os.path.join(os.getcwd(), 'workspace')\n",
    "    !mkdir -p {workspace_dir}\n",
    "    unzip_dir = tempfile.mkdtemp(dir=workspace_dir)\n",
    "    model_folder_name = \"summa_camels\"\n",
    "    model_folder = os.path.join(unzip_dir, model_folder_name)\n",
    "    !unzip -o {model_folder_name}.zip -d {model_folder}\n",
    "    !rm -rf {model_folder}/output\n",
    "    !mkdir {model_folder}/output\n",
    "    !mkdir {model_folder}/output/constant\n",
    "    !mkdir {model_folder}/output/merged_day\n",
    "    !mkdir {model_folder}/output/truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alld = {'groundwatr':np.array(['qTopmodl','bigBuckt'])}\n",
    "alld = {'stomResist':np.sort(np.array(['BallBerry','Jarvis'])),\n",
    "        'snowIncept':np.sort(np.array(['stickySnow', 'lightSnow'])),\n",
    "        'windPrfile':np.sort(np.array(['exponential','logBelowCanopy']))}\n",
    "config_1 = ps.ensemble.decision_product(alld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if config_prob:\n",
    "    import json\n",
    "    with open(os.path.join(model_folder, 'summa_options.json'), 'w') as outfile:\n",
    "        json.dump(config_1, outfile)\n",
    "\n",
    "    # check ensemble parameters    \n",
    "    print(\"Number of ensemble runs: {}\".format(len(config_1)))\n",
    "    print(json.dumps(config_1, indent=4, sort_keys=False)[:800])\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit model to HPC using New Job Submission Service Python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_prob:\n",
    "    from job_supervisor_client import *\n",
    "    communitySummaSession = Session('summa', isJupyter=True)\n",
    "    communitySummaJob = communitySummaSession.job() # create new job\n",
    "    communitySummaJob.upload(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_prob:\n",
    "    communitySummaJob.submit(payload={\n",
    "        \"node\": 8,\n",
    "        \"machine\": \"comet\",\n",
    "        \"file_manager_rel_path\": \"settings/file_manager_truth.txt\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if config_prob:\n",
    "    communitySummaJob.events(liveOutput=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When outputs are very big, sometimes missing data happened. So you need to reexecute this cell again.-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if config_prob:\n",
    "    job_dir = os.path.join(model_folder, \"{}\".format(communitySummaJob.id))\n",
    "    !mkdir -p {job_dir}/output\n",
    "    communitySummaJob.download(job_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_prob:\n",
    "    !cd {job_dir} && unzip *.zip -d output\n",
    "    # check output directory\n",
    "    output_path = os.path.join(job_dir, \"output/truth\")\n",
    "    # check SUMMA output file \n",
    "    name_list = os.listdir(output_path)\n",
    "    full_list1 = [os.path.join(output_path,i) for i in name_list if i.endswith(\".nc\")]\n",
    "    sorted_list = natsorted(full_list1)\n",
    "    sorted_list = natsorted(sorted_list, key=lambda v: v.upper())\n",
    "    for f in sorted_list:\n",
    "        print(f)\n",
    "    print(\"Number of NC files: {}\".format(len(sorted_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# truth, parameter space with default configuration\n",
    "if config_prob==1 and debug==0:\n",
    "    all_ds = [xr.open_dataset(f) for f in sorted_list]\n",
    "    all_name = [n.split(\"_\")[-2] for n in sorted_list]\n",
    "    all_merged = xr.concat(all_ds, pd.Index(all_name, name=\"decision\"))\n",
    "    all_merged.to_netcdf(top_folder+'/output/merged_day/NLDAStruth_configs.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_prob:\n",
    "    for i in range(len(sorted_list)):\n",
    "        shutil.move(sorted_list[i], top_folder+'/output/truth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "To make sure things look how we want, we plot cumulative variables to see how differences are compounding.\n",
    "We plot one HRU (the first one) for 2 months. We are showing winter months, but this can be switched to summer months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cummulative\n",
    "if config_prob==1 and debug==0:\n",
    "    fig, axes = plt.subplots(nrows=14, ncols=1, figsize=(20, 40))\n",
    "    axes = axes.flatten()\n",
    "    axes[0].set_title('Cumulative')\n",
    "\n",
    "    variables = list(all_merged.variables.keys())[7:22]\n",
    "\n",
    "    #start =  24*3*30 #summer\n",
    "    start =  24*9*30 #winter\n",
    "    stop = start + 2*30*24 \n",
    "    truth_plt = all_merged.isel(hru=0, time=slice(start, stop))\n",
    "\n",
    "    for idx, var in enumerate(variables[0:14]):\n",
    "        for i, dec in enumerate(sorted_list[0:8]):    \n",
    "            truth_plt[var].isel(decision=i).plot(ax=axes[idx],label=dec.split(\"/\")[-1].split(\"_\")[2])\n",
    "        axes[idx].set_title('') \n",
    "        axes[idx].set_ylabel(var)\n",
    "        axes[idx].set_xlabel('Date')\n",
    "    plt.tight_layout()\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Finish HPC use  </b> (config_prob) config_prob SUMMA runs (8 simulations) </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Start HPC use  </b> (config_prob) config_prob SUMMA runs with each forcing (56 simulations) </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if config_prob==1 and debug==0:\n",
    "    import tempfile\n",
    "    import shutil, os\n",
    "    workspace_dir = os.path.join(os.getcwd(), 'workspace')\n",
    "    !mkdir -p {workspace_dir}\n",
    "    unzip_dir = tempfile.mkdtemp(dir=workspace_dir)\n",
    "    model_folder_name = \"summa_camels\"\n",
    "    model_folder = os.path.join(unzip_dir, model_folder_name)\n",
    "    !unzip -o {model_folder_name}.zip -d {model_folder}\n",
    "    !rm -rf {model_folder}/output\n",
    "    !mkdir {model_folder}/output\n",
    "    !mkdir {model_folder}/output/constant\n",
    "    !mkdir {model_folder}/output/merged_day\n",
    "    !mkdir {model_folder}/output/truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_2 = {}\n",
    "for k, v in f_config.items():\n",
    "    for k2, v2 in config_1.items():\n",
    "        v_copy = v.copy()\n",
    "        v_copy.update(v2)\n",
    "        config_2[\"{}_{}\".format(k, k2)] = v_copy\n",
    "config_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if config_prob==1 and debug==0:\n",
    "    import json\n",
    "    with open(os.path.join(model_folder, 'summa_options.json'), 'w') as outfile:\n",
    "        json.dump(config_2, outfile)\n",
    "\n",
    "    # check ensemble parameters    \n",
    "    print(\"Number of ensemble runs: {}\".format(len(config_2)))\n",
    "    print(json.dumps(config_2, indent=4, sort_keys=False)[:800])\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit model to HPC using New Job Submission Service Python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_prob==1 and debug==0:\n",
    "    from job_supervisor_client import *\n",
    "    communitySummaSession = Session('summa', isJupyter=True)\n",
    "    communitySummaJob = communitySummaSession.job() # create new job\n",
    "    communitySummaJob.upload(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_prob==1 and debug==0:\n",
    "    communitySummaJob.submit(payload={\n",
    "        \"node\": 56,\n",
    "        \"machine\": \"comet\",\n",
    "        \"file_manager_rel_path\": \"settings/file_manager_constant_airpres.txt\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if config_prob==1 and debug==0:\n",
    "    communitySummaJob.events(liveOutput=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When outputs are very big, sometimes missing data happened. So you need to reexecute this cell again.-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if config_prob==1 and debug==0:\n",
    "    job_dir = os.path.join(model_folder, \"{}\".format(communitySummaJob.id))\n",
    "    !mkdir -p {job_dir}/output\n",
    "    communitySummaJob.download(job_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_prob==1 and debug==0:\n",
    "    !cd {job_dir} && unzip *.zip -d output\n",
    "    # check output directory\n",
    "    output_path = os.path.join(job_dir, \"output/constant\")\n",
    "    # check SUMMA output file \n",
    "    name_list = os.listdir(output_path)\n",
    "    full_list1 = [os.path.join(output_path,i) for i in name_list if i.endswith(\".nc\")]\n",
    "    sorted_list = natsorted(full_list1)\n",
    "    sorted_list = natsorted(sorted_list, key=lambda v: v.upper())\n",
    "    for f in sorted_list:\n",
    "        print(f)\n",
    "    print(\"Number of NC files: {}\".format(len(sorted_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# truth, parameter space with default configuration\n",
    "if config_prob==1 and debug==0:\n",
    "    i = 0\n",
    "    all_ds = [xr.open_dataset(f) for f in sorted_list]\n",
    "    ens_decisions = []\n",
    "    for f in sorted_list:\n",
    "        ens_decisions.append(f.split(\"_\")[-2])\n",
    "    if config_prob==1:\n",
    "        constant_vars= ['airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd']\n",
    "        for v in constant_vars:\n",
    "            all_merged = xr.concat(all_ds[i:i+int(len(sorted_list)/7)], pd.Index(ens_decisions[i:i+int(len(sorted_list)/7)], name=\"decision\"))\n",
    "            all_merged.to_netcdf(top_folder+'/output/merged_day/NLDASconstant_' + v +'_configs.nc')\n",
    "            i = i + int(len(sorted_list)/7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_prob==1 and debug==0:\n",
    "    for i in range(len(sorted_list)):\n",
    "        shutil.copy(sorted_list[i], top_folder+'/output/constant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Finish HPC use  </b> (config_prob) config_prob SUMMA runs with each forcing (56 simulations) </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Run the Full Problem\n",
    "\n",
    "This code will give the full problem, exploring the parameter space and the configurations together.\n",
    "The next notebook in the series runs the most complete figures using this full problem, but parts of the figures can be drawn with the simpler problems. \n",
    "First, we combine the decision sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Start HPC use  </b> (config_latin) config_latin SUMMA runs (88 simulations) </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make ensembles with parameter space (numl parameter sets plus 1 for default), should make 88\n",
    "if lhs_config_prob==1:\n",
    "    config_latin = {}\n",
    "    for key_config in config.keys():\n",
    "        c = config[key_config]\n",
    "        for key_latin in latin.keys():\n",
    "            l = latin[key_latin]\n",
    "            config_latin[key_config+key_latin] = {**c,**l}\n",
    "    print(len(config_latin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_latin1 = json.dumps(config_latin, cls=NumpyEncoder)\n",
    "config_latin = json.loads(config_latin1)\n",
    "config_latin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if lhs_config_prob==1:\n",
    "    import tempfile\n",
    "    import shutil, os\n",
    "    workspace_dir = os.path.join(os.getcwd(), 'workspace')\n",
    "    !mkdir -p {workspace_dir}\n",
    "    unzip_dir = tempfile.mkdtemp(dir=workspace_dir)\n",
    "    model_folder_name = \"summa_camels\"\n",
    "    model_folder = os.path.join(unzip_dir, model_folder_name)\n",
    "    !unzip -o {model_folder_name}.zip -d {model_folder}\n",
    "    !rm -rf {model_folder}/output\n",
    "    !mkdir {model_folder}/output\n",
    "    !mkdir {model_folder}/output/constant\n",
    "    !mkdir {model_folder}/output/merged_day\n",
    "    !mkdir {model_folder}/output/truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if lhs_config_prob==1:\n",
    "    import json\n",
    "    with open(os.path.join(model_folder, 'summa_options.json'), 'w') as outfile:\n",
    "        json.dump(config_latin, outfile)\n",
    "\n",
    "    # check ensemble parameters    \n",
    "    print(\"Number of ensemble runs: {}\".format(len(config_latin)))\n",
    "    print(json.dumps(config_latin, indent=4, sort_keys=True)[:800])\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit model to HPC using New Job Submission Service Python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_config_prob==1:\n",
    "    from job_supervisor_client import *\n",
    "    communitySummaSession = Session('summa', isJupyter=True)\n",
    "    communitySummaJob = communitySummaSession.job() # create new job\n",
    "    communitySummaJob.upload(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_config_prob==1:\n",
    "    communitySummaJob.submit(payload={\n",
    "        \"node\": 88,\n",
    "        \"machine\": \"comet\",\n",
    "        \"file_manager_rel_path\": \"settings/file_manager_truth.txt\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if lhs_config_prob==1:\n",
    "    communitySummaJob.events(liveOutput=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When outputs are very big, sometimes missing data happened. So you need to reexecute this cell again.-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if lhs_config_prob==1:\n",
    "    job_dir = os.path.join(model_folder, \"{}\".format(communitySummaJob.id))\n",
    "    !mkdir -p {job_dir}/output\n",
    "    communitySummaJob.download(job_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_config_prob==1:\n",
    "    !cd {job_dir} && unzip *.zip -d output\n",
    "    # check output directory\n",
    "    output_path = os.path.join(job_dir, \"output/truth\")\n",
    "    # check SUMMA output file \n",
    "    name_list = os.listdir(output_path)\n",
    "    full_list1 = [os.path.join(output_path,i) for i in name_list if i.endswith(\".nc\")]\n",
    "    sorted_list = natsorted(full_list1)\n",
    "    sorted_list = natsorted(sorted_list, key=lambda v: v.upper())\n",
    "    for f in sorted_list:\n",
    "        print(f)\n",
    "    print(\"Number of NC files: {}\".format(len(sorted_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# truth, parameter space with default configuration\n",
    "if lhs_config_prob==1:\n",
    "    all_ds = [xr.open_dataset(f) for f in sorted_list]\n",
    "    all_name = [n.split(\"_\")[-2] for n in sorted_list]\n",
    "    all_merged = xr.concat(all_ds, pd.Index(all_name, name=\"decision\"))\n",
    "    all_merged.to_netcdf(top_folder+'/output/merged_day/NLDAStruth_configs_latin.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_config_prob==1:\n",
    "    for i in range(len(sorted_list)):\n",
    "        shutil.copy(sorted_list[i], top_folder+'/output/truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot cummulative\n",
    "if lhs_config_prob==1:\n",
    "    fig, axes = plt.subplots(nrows=14, ncols=1, figsize=(20, 40))\n",
    "    axes = axes.flatten()\n",
    "    axes[0].set_title('Cumulative')\n",
    "\n",
    "    variables = list(all_merged.variables.keys())[7:22]\n",
    "\n",
    "    #start =  24*3*30 #summer\n",
    "    start =  24*9*30 #winter\n",
    "    stop = start + 2*30*24 \n",
    "    truth_plt = all_merged.isel(hru=0, time=slice(start, stop))\n",
    "\n",
    "    for idx, var in enumerate(variables[0:14]):\n",
    "        for i, dec in enumerate(all_name):    \n",
    "            truth_plt[var].isel(decision=i).plot(ax=axes[idx],label=dec)\n",
    "        axes[idx].set_title('') \n",
    "        axes[idx].set_ylabel(var)\n",
    "        axes[idx].set_xlabel('Date')\n",
    "    plt.tight_layout()\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Finish HPC use  </b> (config_latin) config_prob SUMMA runs (88 simulations) </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Start HPC use  </b> (config_latin) config_latin SUMMA runs with each forcing (616 simulations) </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_vars= ['airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd']\n",
    "config = {}\n",
    "for i, v in enumerate(constant_vars):\n",
    "    key = 'run'+str(i)\n",
    "    value = {'file_manager': '<PWD>/settings/file_manager_constant_' + v +'.txt'}\n",
    "    config[key] = value\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_3 = {}\n",
    "for k, v in config.items():\n",
    "    for k2, v2 in config_latin.items():\n",
    "        v_copy = v.copy()\n",
    "        v_copy.update(v2)\n",
    "        config_3[\"{}_{}\".format(k, k2)] = v_copy\n",
    "config_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_3 = json.dumps(config_3, cls=NumpyEncoder)\n",
    "config_3 = json.loads(config_3)\n",
    "config_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if lhs_config_prob==1:\n",
    "    import tempfile\n",
    "    import shutil, os\n",
    "    workspace_dir = os.path.join(os.getcwd(), 'workspace')\n",
    "    !mkdir -p {workspace_dir}\n",
    "    unzip_dir = tempfile.mkdtemp(dir=workspace_dir)\n",
    "    model_folder_name = \"summa_camels\"\n",
    "    model_folder = os.path.join(unzip_dir, model_folder_name)\n",
    "    !unzip -o {model_folder_name}.zip -d {model_folder}\n",
    "    !rm -rf {model_folder}/output\n",
    "    !mkdir {model_folder}/output\n",
    "    !mkdir {model_folder}/output/constant\n",
    "    !mkdir {model_folder}/output/merged_day\n",
    "    !mkdir {model_folder}/output/truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if lhs_config_prob==1:\n",
    "    import json\n",
    "    with open(os.path.join(model_folder, 'summa_options.json'), 'w') as outfile:\n",
    "        json.dump(config_3, outfile)\n",
    "\n",
    "    # check ensemble parameters    \n",
    "    print(\"Number of ensemble runs: {}\".format(len(config_3)))\n",
    "    print(json.dumps(config_3, indent=4, sort_keys=True)[:800])\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit model to HPC using New Job Submission Service Python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_config_prob==1:\n",
    "    from job_supervisor_client import *\n",
    "    communitySummaSession = Session('summa', isJupyter=True)\n",
    "    communitySummaJob = communitySummaSession.job() # create new job\n",
    "    communitySummaJob.upload(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_config_prob==1:\n",
    "    communitySummaJob.submit(payload={\n",
    "        \"node\": 240,\n",
    "        \"machine\": \"comet\",\n",
    "        \"file_manager_rel_path\": \"settings/file_manager_constant_airpres.txt\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if lhs_config_prob==1:\n",
    "    communitySummaJob.events(liveOutput=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When outputs are very big, sometimes missing data happened. So you need to reexecute this cell again.-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if lhs_config_prob==1:\n",
    "    job_dir = os.path.join(model_folder, \"{}\".format(communitySummaJob.id))\n",
    "    !mkdir -p {job_dir}/output\n",
    "    communitySummaJob.download(job_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_config_prob==1:\n",
    "    !cd {job_dir} && unzip *.zip -d output\n",
    "\n",
    "    # check output directory\n",
    "    output_path = os.path.join(job_dir, \"output/constant\")\n",
    "    # check SUMMA output file \n",
    "    name_list = os.listdir(output_path)\n",
    "    full_list1 = [os.path.join(output_path,i) for i in name_list if i.endswith(\".nc\")]\n",
    "    sorted_list = natsorted(full_list1)\n",
    "    sorted_list = natsorted(sorted_list, key=lambda v: v.upper())\n",
    "    for f in sorted_list:\n",
    "        print(f)\n",
    "    print(\"Number of NC files: {}\".format(len(sorted_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# truth, parameter space with default configuration\n",
    "if lhs_config_prob==1:\n",
    "    i = 0\n",
    "    all_ds = [xr.open_dataset(f) for f in sorted_list]\n",
    "    ens_decisions = []\n",
    "    for f in sorted_list:\n",
    "        ens_decisions.append(f.split(\"_\")[-2])\n",
    "    if lhs_config_prob==1:\n",
    "        constant_vars= ['airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd']\n",
    "        for v in constant_vars:\n",
    "            all_merged = xr.concat(all_ds[i:i+int(len(sorted_list)/7)], pd.Index(ens_decisions[i:i+int(len(sorted_list)/7)], name=\"decision\"))\n",
    "            all_merged.to_netcdf(top_folder+'/output/merged_day/NLDASconstant_' + v +'_configs_latin.nc')\n",
    "            i = i + int(len(sorted_list)/7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_config_prob==1:\n",
    "    for i in range(len(sorted_list)):\n",
    "        shutil.copy(sorted_list[i], top_folder+'/output/constant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Finish HPC use  </b> (config_latin) config_latin SUMMA runs with each forcing (616 simulations) </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysumma",
   "language": "python",
   "name": "pysumma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
